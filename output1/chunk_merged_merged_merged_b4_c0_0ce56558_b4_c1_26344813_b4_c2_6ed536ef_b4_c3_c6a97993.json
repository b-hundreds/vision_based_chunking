{
  "id": "merged_merged_merged_b4_c0_0ce56558_b4_c1_26344813_b4_c2_6ed536ef_b4_c3_c6a97993",
  "content": "F Token Counts Table 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input contexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer, GPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K) use the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer, modulo some additional special tokens that do not appear in our data. As a result, the token counts for these two model families is the same in our experimental settings. | | Closed-Book | Oracle | | :--- | :--- | :--- | :--- | :--- | | | avg \u00b1 stdev | max | avg \u00b1 stdev | max | | LongChat-13B (16K) | 55.6 \u00b1 2.7 | 70 | 219.7 \u00b1 48.5 | 588 | | MPT-30B | 43.5 \u00b1 2.2 | 58 | 187.9 \u00b1 41.8 | 482 | | GPT-3.5-Turbo | 15.3 \u00b1 2.2 | 29 | 156.0 \u00b1 41.8 | 449 | | Claude-1.3 | 15.3 \u00b1 2.2 | 29 | 156.0 \u00b1 41.8 | 449 | Table 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document question answering settings. | | 10 docs | 20 docs | 30 docs | | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | | avg \u00b1 stdev | max | avg \u00b1 stdev | max | avg \u00b1 stdev | max | | LongChat-13B (16K) | 1749.9 \u00b1 112.4 | 2511 | 3464.6 \u00b1 202.3 | 4955 | 5181.9 \u00b1 294.7 | 7729 | | MPT-30B | 1499.7 \u00b1 88.5 | 1907 | 2962.4 \u00b1 158.4 | 3730 | 4426.9 \u00b1 230.5 | 5475 | | GPT-3.5-Turbo | 1475.6 \u00b1 86.5 | 1960 | 2946.2 \u00b1 155.1 | 3920 | 4419.2 \u00b1 226.5 | 6101 | | Claude-1.3 | 1475.6 \u00b1 86.5 | 1960 | 2946.2 \u00b1 155.1 | 3920 | 4419.2 \u00b1 226.5 | 6101 | Table 3: Token count statistics for each of the evaluated models on each of the document question answering settings. | | 75 KV pairs | 140 KV pairs | 300 KV pairs | | :--- | :--- | :--- | :--- | :--- | :--- | :--- | | | avg \u00b1 stdev | max | avg \u00b1 stdev | max | avg \u00b1 stdev | max | | LongChat-13B (16K) | 5444.5 \u00b1 19.1 | 5500 | 10072.4 \u00b1 24.1 | 10139 | 21467.3 \u00b1 35.9 | 21582 | | MPT-30B | 4110.5 \u00b1 23.8 | 4187 | 7609.9 \u00b1 31.1 | 7687 | 16192.4 \u00b1 46.6 | 16319 | | GPT-3.5-Turbo | 3768.7 \u00b1 25.6 | 3844 | 6992.8 \u00b1 34.1 | 7088 | 14929.4 \u00b1 50.7 | 15048 | | Claude-1.3 | 3768.7 \u00b1 25.6 | 3844 | 6992.8 \u00b1 34.1 | 7088 | 14929.4 \u00b1 50.7 | 15048 | Table 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings.",
  "heading_hierarchy": [
    "Lost in the Middle: How Language Models Use Long Contexts",
    "Appendix",
    "F Token Counts"
  ],
  "page_numbers": [
    17,
    18
  ],
  "continuation_flag": "False",
  "source_batch": 4,
  "metadata": {
    "position_in_batch": 0,
    "raw_length": 594,
    "heading_count": 3,
    "merged_from": [
      "merged_merged_b4_c0_0ce56558_b4_c1_26344813_b4_c2_6ed536ef",
      "b4_c3_c6a97993"
    ],
    "original_continuation_flag": "True"
  }
}