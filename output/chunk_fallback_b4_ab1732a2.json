{
  "id": "fallback_b4_ab1732a2",
  "content": "Lost in the Middle: How Language Models Use Long Contexts > Appendix > F Token Counts F Token Counts Tables 2, 3, and Table 4 present the average and maximum number of tokens in each of the input contexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer, GPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K) use the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer, modulo some additional special tokens that do not appear in our data. As a result, the token counts for these two model families is the same in our experimental settings. False </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > F Token Counts Closed-Book Oracle :--- :--- :--- avg \u00b1 stdev max avg \u00b1 stdev max LongChat-13B (16K) 55.6 \u00b1 2.7 70 219.7 \u00b1 48.5 588 MPT-30B 43.5 \u00b1 2.2 58 187.9 \u00b1 41.8 482 GPT-3.5-Turbo 15.3 \u00b1 2.2 29 156.0 \u00b1 41.8 449 Claude-1.3 15.3 \u00b1 2.2 29 156.0 \u00b1 41.8 449 Table 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document question answering settings. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > F Token Counts 10 docs 20 docs 30 docs :--- :--- :--- :--- avg \u00b1 stdev max avg \u00b1 stdev max avg \u00b1 stdev max LongChat-13B (16K) 1749.9 \u00b1 112.4 2511 3464.6 \u00b1 202.3 4955 5181.9 \u00b1 294.7 7729 MPT-30B 1499.7 \u00b1 88.5 1907 2962.4 \u00b1 158.4 3730 4426.9 \u00b1 230.5 5475 GPT-3.5-Turbo 1475.6 \u00b1 86.5 1960 2946.2 \u00b1 155.1 3920 4419.2 \u00b1 226.5 6101 Claude-1.3 1475.6 \u00b1 86.5 1960 2946.2 \u00b1 155.1 3920 4419.2 \u00b1 226.5 6101 Table 3: Token count statistics for each of the evaluated models on each of the document question answering settings. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > F Token Counts 75 KV pairs 140 KV pairs 300 KV pairs :--- :--- :--- :--- avg \u00b1 stdev max avg \u00b1 stdev max avg \u00b1 stdev max LongChat-13B (16K) 5444.5 \u00b1 19.1 5500 10072.4 \u00b1 24.1 10139 21467.3 \u00b1 35.9 21582 MPT-30B 4110.5 \u00b1 23.8 4187 7609.9 \u00b1 31.1 7687 16192.4 \u00b1 46.6 16319 GPT-3.5-Turbo 3768.7 \u00b1 23.6 3844 6992.8 \u00b1 34.1 7088 14929.4 \u00b1 50.7 15048 Claude-1.3 3768.7 \u00b1 23.6 3844 6992.8 \u00b1 34.1 7088 14929.4 \u00b1 50.7 15048 Table 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > G Full Multi-Document Question Answering Results G Full Multi-Document Question Answering Results This section tabulates model performance when evaluated on the multi-document QA task with varying numbers of documents (Figure 5). \u201cIndex k\u201d indicates performance when the document with the answer occurs at position n = k, where lower indices are closer to the start of the input context. For example, index 0 refers to performance when the document with the answer is placed at the very start of the context (i.e., first amongst all documents). False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > G Full Multi-Document Question Answering Results > G.1 10 Total Retrieved Documents Model Index 0 Index 4 Index 9 :--- :--- :--- :--- Claude-1.3 62.9% 58.3% 59.7% Claude-1.3 (100K) 63.1% 58.3% 59.7% GPT-3.5-Turbo 76.9% 61.2% 62.4% GPT-3.5-Turbo (16K) 76.9% 61.0% 62.5% MPT-30B-Instruct 60.2% 56.2% 59.7% LongChat-13B (16K) 72.1% 58.9% 58.5% Table 5: Model performance when evaluated on the multi-document QA task with 10 total retrieved documents. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > G Full Multi-Document Question Answering Results > G.2 20 Total Retrieved Documents Model Index 0 Index 4 Index 9 Index 14 Index 19 :--- :--- :--- :--- :--- :--- Claude-1.3 59.9% 55.9% 56.8% 57.2% 60.1% Claude-1.3 (100K) 59.8% 55.9% 57.0% 57.4% 60.0% GPT-3.5-Turbo 75.8% 57.2% 53.8% 54.4% 63.2% GPT-3.5-Turbo (16K) 75.7% 57.3% 54.1% 55.4% 63.1% MPT-30B-Instruct 53.7% 51.8% 52.2% 52.7% 56.3% LongChat-13B (16K) 68.6% 57.4% 55.3% 52.5% 55.0% Table 6: Model performance when evaluated on the multi-document QA task with 20 total retrieved documents. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > G Full Multi-Document Question Answering Results > G.3 30 Total Retrieved Documents Model Index 0 Index 4 Index 9 Index 14 Index 19 Index 24 Index 29 :--- :--- :--- :--- :--- :--- :--- :--- Claude-1.3 59.1% 55.1% 54.8% 55.7% 56.4% 56.2% 59.9% Claude-1.3 (100K) 59.1% 55.1% 54.9% 55.7% 56.6% 56.1% 60.0% GPT-3.5-Turbo (16K) 74.4% 55.1% 50.5% 50.9% 51.8% 54.9% 63.7% MPT-30B-Instruct 51.6% 51.7% 51.2% 49.0% 49.2% 51.3% 55.1% LongChat-13B (16K) 66.9% 54.8% 52.5% 52.9% 52.2% 51.3% 55.1% Table 7: Model performance when evaluated on the multi-document QA task with 30 total retrieved documents. False</CONTINUES] </CHUNK>",
  "heading_hierarchy": [
    "Fallback Content"
  ],
  "page_numbers": [
    17,
    18
  ],
  "continuation_flag": "False",
  "source_batch": 4,
  "metadata": {
    "is_fallback": true,
    "raw_length": 5894
  }
}