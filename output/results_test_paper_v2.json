{
  "document_name": "test_paper.pdf",
  "total_pages": 6,
  "chunks": [
    {
      "id": "90178558-6f5c-4735-be88-bf6b4f3c5caf",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Title and Authors > Title and Authors",
      "content": "Do Large Language Models Need a Content Delivery Network?\n\nYihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang\nThe University of Chicago",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Title and Authors",
      "chunk_title": "Title and Authors",
      "page_numbers": [
        1
      ]
    },
    {
      "id": "81af66aa-69d0-4ab9-ad24-6b6e126b6188",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Abstract > Abstract",
      "content": "As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling modular and efficient injection of new knowledge in LLM inference is critical. We argue that compared to the more popular fine-tuning and in-context learning, using KV caches as the medium of knowledge could simultaneously improve the modularity of knowledge injection and the efficiency of LLM serving with low cost and fast response. To make it practical, we envision a Knowledge-Delivery Network (KDN), a new component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. Just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. An open-sourced KDN prototype: https://github.com/LMCache/LMCache.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Abstract",
      "chunk_title": "Abstract",
      "page_numbers": [
        2
      ]
    },
    {
      "id": "c9c40b7c-3396-4921-bdcf-208a71d5d3e7",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > Introduction to Knowledge Injection in LLMs",
      "content": "**1 Background and Motivation**\n\nTraditionally, machine learning models, such as computer vision [22, 30, 31, 38] and image generation [20, 32, 37], learn all the knowledge from the training data. However, with the rapid usage growth of large language models (LLMs), applications require the ability to inject external knowledge, unseen in training, into LLM inference. For instance, chatbots [1, 5, 16] and agent systems use the chatting history as supplementary knowledge to generate personalized responses; enterprise-use LLMs generate answers to queries based on their internal databases, as new knowledge, using retrieval-augmented generation (RAG) [19, 25, 27, 39]; and searching engines use LLM to read fresh and relevant web data from the Internet for each user query [2, 6, 10].\nNot only is providing more external knowledge among the most efficient ways to boost LLM quality [17, 28, 39, 41], but the requirements of knowledge injection also become more complex and diverse. For instance, in enterprise settings, data scientists and application operators often demand the flexibility that allows them to directly specify which documents (or which parts of a document) should or should not be used as the context to answer a given query [4, 9]. Moreover, as new data is being constantly collected [3, 8], the knowledge injection method must be efficient enough to keep up with the rapidly evolving pool of knowledge.\n\nThe 2nd Workshop on Hot Topics in System Infrastructure (HotInfra’24), co-located with SOSP’24, November 3, 2024, Austin, TX, USA",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "Introduction to Knowledge Injection in LLMs",
      "page_numbers": [
        3,
        4
      ]
    },
    {
      "id": "274a2b61-49b2-48ea-bc10-5832923822d1",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > The Problem of How to Inject Knowledge",
      "content": "Thus, the key question is “how to design and implement a system to inject knowledge to LLMs?”\n\nThree knowledge-injection methods exist. In-context learning and fine-tuning are the two popular options employed by knowledge-augmented generation. Fine-tuning retrains an LLM on a new dataset using a set of labeled knowledge to update all or parts of the model’s parameters. Unlike fine-tuning, in-context learning leverages the existing capabilities of the model to interpret and respond to new information, without altering the model’s parameters. Yet, compared to using a fine-tuned model, in-context learning has much higher run-time computation overhead as it has to process a much longer input (i.e., the prefill step), before the model can generate the first token.\n\nAn alternative, which we refer to as KV-cache learning, is to let the LLM pre-compute the KV cache¹ of the new knowledge text so that when the same knowledge is needed to supplement another LLM query, the KV cache can be directly used by LLM. This way, LLMs can directly generate the response fast as the fine-tuned model, with little extra computation overhead to prefill the text of the knowledge as in-context learning. However, KV-cache learning, with a straightforward implementation, will suffer from the large size of the KV caches.\nMost research in the machine-learning communities has primarily focused on the generation quality of different knowledge-injection methods, in F1 score and accuracy [15, 29, 35]. Studies have shown that both in-context learning and fine-tuning can achieve high text-generation quality if they are configured appropriately by machine-learning engineers [11, 21, 44]. However, less is known about the tradeoff of these methods presented to system engineers who implement and maintain the LLM infrastructure for knowledge-augmented LLM applications.\n\nThis paper sheds light on these methods from a *system architecture* perspective (Figure 1). Specifically, we make two key arguments.\n*   *Existing knowledge-injection methods–in-context learning, fine-tuning, and KV-cache learning–mainly differ in the tradeoffs between their* **modularity** (ease of adding new knowledge and flexibility to specify injected knowledge) and **efficiency** (in per-query cost and response delay). (§2)\n*   *Compared to in-context learning and fine-tuning, KV-cache learning* **could** *improve both modularity and efficiency.*\n\n¹KV cache is the intermediate state when LLM prefill on a text [33, 34, 40], which represents the LLM’s understanding of the knowledge.\n¹Architecture means the interfaces between the modules, and we leave the refinement of implementation to future work.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "The Problem of How to Inject Knowledge",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "05a44138-f6af-42c2-9b3d-852d1f3ce9dc",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > Figure 1: Trade-offs Between Knowledge-Injection Methods",
      "content": "Figure 1. Knowledge-injection methods make trade-offs between modularity and efficiency. With Knowledge Delivery Networks (KDNs), KV-cache learning can improve on both dimensions compared to in-context learning and fine-tuning.\n\nThis visualization is a table that compares three knowledge-injection methods based on two system-centric metrics: Modularity and Efficiency.\n- **In-context learning:** Has high modularity (indicated by a green checkmark) but low efficiency (indicated by a red cross). This means it's easy to add new knowledge, but it is computationally expensive and slow.\n- **Fine-tuning:** Has low modularity (red cross) but high efficiency (green checkmark). This means it is efficient at inference time, but incorporating new knowledge is a complex and inflexible process.\n- **KV-cache learning (with Knowledge Delivery Networks):** Is shown to have both high modularity (green checkmark) and high efficiency (green checkmark). The figure proposes that this method resolves the trade-off, offering the benefits of both other approaches.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "Figure 1: Trade-offs Between Knowledge-Injection Methods",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "61d061b3-f72f-4f39-abca-77c840e45d6a",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > Introducing the Knowledge-Delivery Network (KDN)",
      "content": "If a new system component, called **knowledge-delivery network (KDN)**, optimizes the management of KV caches by harnessing several emerging research efforts. (§3.2)\n\nAt a high level, a KDN serves as a back-end of LLM-processed knowledge (i.e., KV caches), which can be a part of an LLM-serving system or shared across multiple LLM-serving systems. Unlike the existing LLM-serving systems [23, 26, 46], which deeply couple KV caches with LLM engines (i.e., sharing KV caches with GPUs and managing KV caches inside inferencing), KDNs call for a clean **separation** between KV-cache management and LLM serving engines, for better modularity and efficiency.\n\nWe will outline the key components of a KDN, including a storage pool of KV caches that leverages KV-cache compression, a fast KV-cache streaming system to transfer KV caches between LLM serving engines, and a KV-cache blending module that dynamically puts together multiple pieces of knowledge stored in modularized KV caches. Using a prototype of KDN, we will show that some emerging research efforts have already provided preliminary techniques, which together can make highly efficient KDNs a reality.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "Introducing the Knowledge-Delivery Network (KDN)",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "ee55e910-8a4e-4360-b1f1-c7edd4a1a5a0",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From A System Perspective > Overview",
      "content": "**2 LLM Knowledge-Injection From A System Perspective**\n\nKnowledge-augmented generation, particularly, fine-tuning and in-context learning, is well-studied in the literature. Fine-tuning embeds a corpus of text in the LLM’s weights, so the fine-tuned model can directly respond to a user query with a low response delay. However, as the entire corpus of texts must be embedded in the model together, fine-tuning lacks the flexibility to add new knowledge and specify what knowledge should or should not be used.\n\nIn-context learning is the opposite of fine-tuning, as it allows the operators to specify which external knowledge should be used easily by putting the texts in the LLM input. However, the compute overhead of prefilling will grow superlinearly with the input length, causing a long response delay when more external data is added to the input.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From A System Perspective",
      "chunk_title": "Overview",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "a46a8bed-81ad-4190-9734-dbc9d262e5ba",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From A System Perspective > The efficiency-modularity tradeoff",
      "content": "**2.1 The efficiency-modularity tradeoff**\n\nBoth methods can achieve similar text-generation quality if used in the right way [11, 21, 44]. Instead of viewing these options only in terms of accuracy (i.e., through the ML perspective), we compare the knowledge-injection methods along the following two system-centric metrics: **modularity** and **efficiency**.\n\n**Modularity:** In the context of knowledge-augmented LLM, the modularity of a method includes two aspects. First, a modular approach should allow service providers to specify which knowledge to use and compose them easily. Second, the overhead (e.g., time, cost) of injecting new knowledge into the LLM should be minimized.\nIn-context learning puts the new knowledge in the model’s input, rather than the model itself. The separation of knowledge and model serves as the key to modularity–LLM service providers can specify which knowledge to use and easily compose different pieces of knowledge, which helps the LLM to avoid conflicting knowledge and improve the generation quality. In contrast, fine-tuning has poor modularity. Users cannot specify which knowledge in the fine-tuned model would be used to generate the answer. Moreover, fine-tuning needs to happen for every new knowledge and model, which may take hours to days to complete.\n\n**Efficiency:** On the other hand, the efficiency of a knowledge-augmented LLM system is measured by per-query cost and response delay during LLM inference. Cost is the computation needed for the LLM to handle a request, and response delay is defined as the time between the LLM serving having a request and the generation of the first token. Viewed through this lens, in-context learning is not ideal, because when using in-context learning, LLMs must spend a long time prefilling input text with the knowledge before generating the first token. In contrast, fine-tuning is better in terms of efficiency. Because the knowledge is embedded in the model’s weights, the fine-tuned model can skip the long prefill.\n\nIn short, in-context learning is more modular but sacrifices efficiency, while fine-tuning, though achieving better efficiency, suffers from the overhead of incorporating and controlling the knowledge due to poor modularity.\n**2.2 KV-Cache Learning**\n\nAlternatively, KV-cache learning stores the knowledge in a KV-generated KV cache, and injects knowledge by feeding the KV cache to the LLM, without modifying the model. A KV cache stores the knowledge in the form of the attention state generated by the model after it processes the text, so the KV cache, once generated, can be reused by the LLM to skip prefilling if the subsequent requests use the same context. When many queries reuse the same context, reusing its KV cache could reduce the per-query delay and compute usage, while still preserving the modularity as in-context learning. The idea of KV-cache learning has gained increasing attention in LLM services (e.g., [7, 36, 45]).\n**Why storing knowledge in KV caches pays off?** On the surface, the use of KV caches may seem merely a space-time tradeoff (trading KV cache storage space for shorter prefill), but the tradeoff is favorable for two reasons: might degrade, lowering the inference quality. Thus, when the KV caches are reused by more queries repeatedly, the degraded quality will also affect more queries.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From A System Perspective",
      "chunk_title": "The efficiency-modularity tradeoff",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "468a2650-cca8-4d9c-8ab3-6ce2169a51ba",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From A System Perspective > Figure 2: Architecture of a Knowledge Delivery Network (KDN)",
      "content": "Figure 2. Architecture of a Knowledge Delivery Network (KDN)\n\nThis figure illustrates the proposed architecture of a Knowledge Delivery Network (KDN) and contrasts it with traditional in-context learning.\n\n**(a) Injecting knowledge via in-context learning (feeding text or raw data to LLMs):** This diagram shows various data sources (Chat history, Books, News, Docs, Video) being fed directly into \"LLM serving engines\". This represents the standard approach where raw text is included in the prompt for each query.\n\n**(b) Injecting knowledge via Knowledge-Delivery Networks (feeding KV caches to LLMs):** This diagram shows the same data sources feeding into a central \"Knowledge Delivery Network (KDN)\". The KDN processes this information and then provides it to the \"LLM serving engines\". This illustrates the proposed system where knowledge is pre-processed into KV caches by the KDN before being sent to the LLM.\n\n**(c) High-level structure of the proposed Knowledge Delivery Network (KDN):** This diagram details the internal components of the KDN. It consists of three main parts:\n1.  **KV Cache Store:** This component is responsible for storing the KV caches. It aims for \"Better long-term content quality via opportunistic attention steering\".\n2.  **KV Cache Delivery:** This component handles the transfer of KV caches. Its goal is \"Fast KV cache delivery via encoding & streaming\".\n3.  **KV Cache Blend:** This component combines different KV caches. Its function is \"KV cache composition via selective recomposition\".\n\nThe overall architecture proposes a modular system that separates knowledge processing (KDN) from the core LLM inference engine, aiming to improve efficiency and reusability.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From A System Perspective",
      "chunk_title": "Figure 2: Architecture of a Knowledge Delivery Network (KDN)",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "c9a650a7-f458-43b0-acb1-7979b4cd8c45",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From A System Perspective > Challenges of KV Cache Reuse",
      "content": "*   *KV caches are reused a lot.* Many contexts, especially long contexts, are frequently reused for different queries and different users. This can be viewed as the LLM version of the Pareto’s rule: an LLM, like human, uses 20% of the knowledge for 80% of the time, which means knowledge is frequently reused. Just consider that if a user asks the LLM to read a book, it is unlikely that they will only ask one book-related question.\n*   *The size of KV caches increases slower than prefill delay.* As the context increases, the KV cache size grows linearly, whereas the prefill delay grows superlinearly. And, as the LLM gets bigger, more compute will happen at the feedforward layers which do not affect the KV cache size.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From A System Perspective",
      "chunk_title": "Challenges of KV Cache Reuse",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "b642f8aa-5738-41e1-8e34-16be57d0e82c",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge Injection > Limitations of Existing KV-Cache Systems",
      "content": "**3 Knowledge Delivery Networks for Efficient Knowledge Injection**\n\nLimitations of Existing KV-Cache Systems: Despite the promise, existing KV-caching learning systems still have some technical limitations.\n*   *Limited storage for KV caches:* Currently, many serving engines only use the fast CPU/GPU memory, locally accessible by an individual LLM instance, to store KV caches. Such local memory only storage greatly limits the amount of KV caches that are stored and reused. For instance, a CPU memory of 64 GB can only store the KV cache of 160k tokens (two pdf reports) for a small-size LLM (Llama-34B). However, expanding the storage of KV caches to disk or remote servers would significantly constrain the bandwidth for loading the KV caches into GPU memory.\n*   *Prefix-only caches:* To reuse the KV cache, most systems require that the text of the KV cache must be the *prefix* of the LLM input. Even though reusing the KV cache of the prefix is naturally supported by LLMs, this assumption of “sharing prefix only” severely limits its use cases. For instance, retrieval-augmented generation (RAG) concatenates multiple retrieved text chunks in the LLM input, so most retrieved text will not be the prefix.\n*   *Degraded quality with long contexts.* Finally, as more texts are added to the input as LLM’s context (i.e., long context), the LLM’s capability to capture important information",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Limitations of Existing KV-Cache Systems",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "8c877574-464a-4263-ae6f-75de47812191",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge Injection > Knowledge Delivery Architecture",
      "content": "**3.1 Knowledge Delivery Architecture**\n\nAt first glance, these challenges facing prior KV-cache-based systems may look disparate. Yet, our insight is that they share a common need—*a separate KV-cache management system, which dynamically compresses, composes, and modifies KV caches to optimize the storage and delivery of KV caches and the LLM inference based on KV caches.* We refer to such a system as a **Knowledge Delivery Network (KDN)**. As depicted in Figure 2, the envisioned architecture of a KDN consists of three main modules:\n*   The **storage** module stores the KV caches keyed by various texts and offline modifies the KV cache content such that the inference quality will be improved when the KV caches are fed to the LLM.\n*   The **delivery** module transmits the compressed KV caches from the storage device to the server running the LLM and decompresses them in GPU to be used by the LLM serving engine.\n*   The **blending** module dynamically composes multiple KV caches corresponding to different texts when these texts are put together in the context of an LLM query.\n\nThe existing LLM serving systems (e.g., [7, 46]) deeply couple KV caches with LLM engines. In contrast, the concept of KDN is to **separate** the management of KV caches and the LLM serving engines. Such separation will enable innovations on the storage, sharing, and use of KV caches, without needing to be deeply coupled with the LLM serving engines. By implementing these optimizations separately, a KDN serves as a critical sub-system module for LLM-serving ecosystems. It enables the decoupling of KV-cache management from LLM serving engines, which allows LLM serving engines to focus on fast query-driven inference, while the KDN can focus on the KV-cache-related optimizations independently.\n**3.2 Technical Roadmap**\n\nThe architecture of KDN itself does not directly address the challenges associated with KV caches; it merely breaks a\npotential solution into three modules. Fortunately, we observe that emerging research efforts could lead to a sufficient design for each module, thus making KDN practical.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Knowledge Delivery Architecture",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "c0c7b8f7-2ae2-40b9-84ef-9f43fac3ed1a",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge injection > Table 1: Comparison of Different Knowledge-Injection Methods",
      "content": "Table 1. Comparison between different knowledge-injection methods under a RAG setup. With KDN, KV-cache learning is 40× faster when incorporating new knowledge than fine-tuning, and achieves 3.7× faster and 2.5× cheaper during inference compared to in-context learning.\n\nThis table compares three knowledge-injection methods—Fine-tuning, In-context learning, and KV-cache learning with a KDN—across three metrics related to Modularity and Efficiency.\n\n| | **Modularity** | **Efficiency** | |\n| :--- | :--- | :--- | :--- |\n| | **Time to inject new knowledge** | **Inference cost ($) per request** | **Response delay (s) per request** |\n| **Fine-tuning** | 10 hours | 0.0022 | 0.21 |\n| **In-context learning** | 0 | 0.0149 | 10.91 |\n| **KV-cache learning w/ KDN** | 0.25 hours | 0.0059 | 2.97 |\n\n**Interpretation:**\n*   **Fine-tuning:** Is extremely slow to incorporate new knowledge (10 hours) but is very efficient at inference time, with the lowest cost ($0.0022) and fastest response (0.21s). This highlights its low modularity but high efficiency.\n*   **In-context learning:** Is instantly modular (0 time to inject knowledge) but is the least efficient, with the highest inference cost ($0.0149) and a very long response delay (10.91s).\n*   **KV-cache learning w/ KDN:** Strikes a balance. It is significantly faster than fine-tuning for knowledge injection (0.25 hours vs. 10 hours). While not as cheap or fast as a fully fine-tuned model at inference, it is substantially more efficient than in-context learning, being 2.5x cheaper and 3.7x faster. This data supports the paper's central claim that KDNs can improve both modularity and efficiency compared to the other methods.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge injection",
      "chunk_title": "Table 1: Comparison of Different Knowledge-Injection Methods",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "e6ab0cd7-e07c-436f-8fdf-33c978358900",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge injection > KV-cache delivery",
      "content": "**KV-cache delivery:** The recent KV cache compression techniques make it possible to cheaply store and quickly load KV caches outside GPU and CPU memory. For instance, Ya-Lin [33] compresses the KV cache by quantizing and then encoding it into binary strings. LLMLingua [24] introduces a smaller language model to identify and remove non-essential tokens in the knowledge’s text, thus reducing the size of the corresponding KV cache. H2O [43] directly removes elements in the KV cache based on their importance calculated during inference. By combining the above techniques, the memory footprint of the KV cache can be reduced by over 10×, drastically improving the loading speed and the storage cost of the KV cache.\n\nKV-cache blending: Recent works also improve the composability of the KV caches. CacheBlend [45], for instance, enables arbitrarily composing different KV caches by recomputing the cross-attention between KV caches, where the recomputation only needs 10% computation of prefilling the full text. PromptCache [18] lets users define a prompt template with different segments, which allows each segment’s KV cache to be reused at different positions rather than prefix-only.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge injection",
      "chunk_title": "KV-cache delivery",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "05455fdb-8896-4711-a318-0a2bbf09bcae",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge injection > Offline KV-cache editing and LLM serving engines",
      "content": "**Offline KV-cache editing:** By separating KV-cache management from the LLM-serving engines, KDNs open up new possibilities to improve inference quality. Recent works have shown that if the attention matrix of an LLM input is appropriately modified, the inference quality can significantly improve [13, 42]. In other words, when a KV cache is “de-posited” to KDN, KDN not only stores it but also can actively influence the LLM inference quality by offline editing the KV cache and returning the edited KV cache when it is retrieved next time, all of which is done without any change to the model itself or the input prompt.\n\n**Interface with LLM serving engines:** Currently, most LLM serving engines, such as vLLM [23], HuggingFace TGI [23], and SGLang [46], do not readily support the injection of external provided KV caches as a part of the LLM input. Instead, their internal implementation for KV-cache management (e.g., paged memory in vLLM) is deeply cobbled with the model inference implementation. One way to deploy KDN is to augment each LLM engine with a KDN as submodule of the LLM engine. However, as elaborated in §3.1, developing a performant KDN is a substantial undertaking, so it will create much redundant engineering effort if each LLM engine maintains and evolves its KDNs. To avoid reinventing the wheel, the LLM serving engines could interface with a separate, shared KDN provider via two APIs: (i) the LLM stores the KV cache with the associated text to KDN, and (ii) the LLM retrieves the KV cache from KDN using some text. Exposing these APIs is feasible, given most popular LLM engines are open-source. Still, several design questions remain: how to leverage heterogeneous links, such as NVLink, RDMA, or PCIe, to transfer KV caches? Can the APIs be shared with other LLM functions, like disaggregated prefilling?",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge injection",
      "chunk_title": "Offline KV-cache editing and LLM serving engines",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "72bee7d7-a708-4c65-a403-7d1634b6a200",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge injection > Early Promise",
      "content": "**Early promise:** Based on these techniques, we implemented LMCache (https://github.com/LMCache/LMCache), a prototype of KDNs. We compare the modularity and efficiency of KV-cache learning with KDN to fine-tuning and in-context learning, under a RAG use case with the following setup:\n*   Total size of knowledge (in text): 2 million tokens.\n*   Each request: 8k tokens knowledge plus 2k tokens user chatting history.\n*   Language model: Llama 3-L70B [18].\n*   Hardware: 2 Nvidia A40 GPUs.\n\nModularity is measured by the time² of injecting the knowledge base into LLM, and efficiency is measured by inference cost³ and response delay per request. Table 1 shows that with the KDN, KV-cache learning can be 40× faster than fine-tuning when injecting new knowledge, and it also achieves 3.7× cheaper and 2.5× faster during inference time compared to in-context learning.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge injection",
      "chunk_title": "Early Promise",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "d7519151-e2c6-4cac-8508-44fa7d82194f",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Conclusion > Conclusion",
      "content": "**4 Conclusion**\n\nIn short, this paper makes a case for (1) the separation between the management of knowledge in the form of KV caches and LLM serving engines, and (2) a Knowledge-Delivery Network (KDN) as a new LLM system component that integrates recent research developments to optimize the efficiency (in speed and cost) of KV-cache-based knowledge injection. We hope this paper can inspire more research to tackle the aforementioned problems.\n\n²We assume the chatting history cannot be pre-learned by fine-tuning.\n²The time for fine-tuning the model is estimated from Llama-Adapter [15].\n³The inference cost is calculated based on the AWS cloud price [12].",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Conclusion",
      "chunk_title": "Conclusion",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "29842a6d-82c4-4ceb-aaf8-75e2a4c1f9e7",
      "heading": "Do Large Language Models Need a Content Delivery Network? > References > Full List of References",
      "content": "**References**\n\n[1] character.ai | personalized ai for every moment of your day. https://character.ai/. (Accessed on 09/07/2024).\n\n[2] Enterprise search: an llm-enabled out-of-the-box search engine. https://io.google/2023/program/27ce05f-df4c-4ab2-9a59-5b46b6daef09/. (Accessed on 09/07/2024).\n\n[3] How many websites are there in the world? (2024) – siteefy. https://siteefy.com/how-many-websites-are-there/. (Accessed on 09/08/2024).\n\n[4] Instruction to exclude certain information when generating answer - opensearch documentation. https://opensearch.org/docs/latest/ml-commons-plugin/exclude-certain-information-when-generating-answer/#470451. (Accessed on 09/08/2024).\n\n[5] Introducing the chatgpt api. https://openai.com/index/chatgpt/. (Accessed on 09/07/2024).\n\n[6] Perplexity. https://www.perplexity.ai. (Accessed on 09/07/2024).\n\n[7] Perplexity. https://labs.perplexity.ai/l/2024-09-17-release. (Accessed on 09/14/2024).\n\n[8] Perplexity partners with elevenlabs to launch ‘discover daily’ podcast. https://www.perplexity.ai/hub/blog/perplexity-partners-with-elevenlabs-to-launch-discover-daily-podcast. (Accessed on 09/08/2024).\n\n[9] Revolutionizing operational efficiency: Unifying analytics and observability for seamless decision-making. https://www.confluent.io/blog/revolutionizing-operational-efficiency-unifying-analytics-and-observability-for-seamless-decision-making/. (Accessed on 09/07/2024).\n\n[10] Appsearch: a search engine for search. https://consensus.app/search/. (Accessed on 09/07/2024).\n\n[11] Simone Aghina, Massimo Rizzati, Gabriele Bordonaro, Seyed Mahdi Hashemi, and Giacomo Giudice. Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue. arXiv preprint arXiv:2404.08848, 2024.\n\n[12] Amazon Web Services. Ec2 on-demand instance pricing – amazon web services, 2024. (Accessed on September 02, 2024).\n\n[13] Anonymous. Model full where fine-tuning and faithfulness meets automatic attention steering. In Submitted to ACL Rolling Review - June 2024. ACL, Under review.\n\n[14] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 17754–17762, 2023.\n\n[15] Yixuan Su, Tianle Zhang, Sheng Zhe, George Karypis, and He He. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814, 2021.\n\n[16] Panos Georgiou, Giannis Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, George E. Dahl, et al. In-context reinforcement learning from a evaluating limits to human preference. arXiv preprint arXiv:2403.04132, 2024.\n\n[17] Hongxia Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2023.\n\n[18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Ahmed, Akhil Mathur, Alon Schkler, Amy Tung, Anand Iyer, et al. The little book of llm tricks. arXiv preprint arXiv:2407.1783, 2024.\n\n[19] In-Hwan Jeong, Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khan, Delwal, and Lin Zhang. Prompt cache: Modular attention reuse for low-latency inference, 2023.\n\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n\n[21] Anirvy Ghiya, Anup Shirgaonkar, Anup de Luis Balaguer, Bruno Guis, Ankit Holkar, et al. Unifying fine-tuning and retrieval: O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. arXiv preprint arXiv:2401.08406, 2024.\n\n[22] Kaiyang He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n\n[23] Huggingface. text-generation-inference, 2024.\n\n[24] Huayang Li, Junwei Deng, Qihang Wu, Yixuan Su, Yiran Wang, and Yidi Liu. Longllm: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023.\n\n[25] Zhenghao Jiang, Faezeh Jafari, Jinyang Li, Zhaoyang Sun, Qiao Jin, Jiacheng Liu, Pengfei Yang, Jamin Callahan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023.\n\n[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474, 2020.\n\n[27] Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474, 2020.\n\n[28] Hongjin Liu, Weitao Li, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge bases. arXiv preprint arXiv:2401.10446, 2024.\n\n[29] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Staying on topic with llms: fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1946–1962, 2022.\n\n[30] Bo Liu, Hanzi Wang, Chun-Li Zhang, and Wei Liu. Visual instruction tuning. Advances in neural information processing systems, 36, 2023.\n\n[31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14, pages 21–37. Springer, 2016.\n\n[32] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Mu Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024.\n\n[33] Yuhuai Wu, Shucheng Li, Jianmin Du, Jiao Guo, Yilun Liu, Yuyang Zhang, Shan Liu, Michael Maire, Elliot H Hoffman, Ari Holtzman, et al. Cachegen: Fast context loading for language model applications. arXiv preprint arXiv:2310.05292, 2023.\n\n[34] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Jianzong Wu, Xiaodong Kyriakis, and Anastasios Kyrillidis. The unbribable: Unveiling the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2023.\n\n[35] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023.\n\n[36] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Meixu Xu. Me-cache: Build a kvcache-centric.\narchitecture for llm serving. arXiv preprint arXiv:2407.00079, 2024.\n\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n\n[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n\n[39] Zhaoyang Zhang, Yifan Xing, Yixuan Wei, Hanxue Liang, Yutong Lin, Hanqing Wang, Weizhe Liu, Jianhan Xu, Yanzhao Yang, Xuebo Liu, Hanqiu Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Efficient and effective object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10781–10790, 2022.\n\n[40] Jinyi Hu, Xiaochen Shen, Li-Jia Li, Siddhartha Rao, Yihua Cheng, Yuyang Zhang, Kuntai Du, Shan Liu, and Jianchen Jiang. Cachekv: Fast large language model serving with cached knowledge fusion. arXiv preprint arXiv:2407.12611, 2024.\n\n[41] Zhenrui Yue, Honglei Zhuang, Ajihan Bai, Kai Hui, Rolf Jagerman, Hanjo Gong, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Efficient scaling for large language model augmented generation. arXiv preprint arXiv:2410.04343, 2024.\n\n[42] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention steering for llms. arXiv preprint arXiv:2310.02538, 2023.\n\n[43] Zhenrui Zhang, Zhaoning Ruan, Chuan Fu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongzheng Li, and Xin Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.\n\n[44] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131, 2024.\n\n[45] Yilei Zheng, Yue-Kai Zhang, Yuxuan Liang, Zhuo Li, Jianfei Chen, Jianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangqing Wang, and Beidi Chen. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. In Workshop on Efficient Systems for Foundation Models (SysML2023), 2023.\n\n[46] Ying-Cong Zheng, Zhengping Zhou, Tim, Zhiyuan Liu, Jeff Huang, Chuyang Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Efficiently programming large language models using sglang. arXiv preprint arXiv:2312.07104, 2023.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "References",
      "chunk_title": "Full List of References",
      "page_numbers": [
        5,
        6
      ]
    }
  ]
}