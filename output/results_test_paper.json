{
  "document_name": "test_paper.pdf",
  "total_pages": 6,
  "chunks": [
    {
      "id": "c2241eae-861e-4600-b4b1-7419f424a622",
      "heading": "Do Large Language Models Need a Content Delivery Network? > [Preamble] > Title and Authors",
      "content": "Do Large Language Models Need a Content Delivery Network?\n\nYihua Cheng, Kuntal Du, Jiayi Yao, Junchen Jiang\nThe University of Chicago",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "[Preamble]",
      "chunk_title": "Title and Authors",
      "page_numbers": [
        1
      ]
    },
    {
      "id": "6242096b-4d25-4eca-b229-50f4e62ff4a4",
      "heading": "Do Large Language Models Need a Content Delivery Network? > [Preamble] > Abstract",
      "content": "**Abstract**\n\nAs the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling modular and efficient injection of new knowledge in LLM inference is critical. We argue that compared to the more popular fine-tuning and in-context learning, using KV caches as the medium of knowledge could simultaneously improve the modularity of knowledge injection and the efficiency of LLM serving with low cost and fast response. To make it practical, we envision a Knowledge-Delivery Network (KDN), a new component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. Just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. An open-sourced KDN prototype: https://github.com/LMCache/LMCache.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "[Preamble]",
      "chunk_title": "Abstract",
      "page_numbers": [
        2
      ]
    },
    {
      "id": "606adc78-ce1b-4500-a4f4-f44e2c299fc0",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 1 Background and Motivation > Introduction and Key Arguments",
      "content": "**1 Background and Motivation**\n\nTraditionally, machine learning models, such as computer vision [22, 30, 31, 38] and image generation [20, 32, 37], learn all the knowledge from the training data. However, with the rapid usage growth of large language models (LLMs), applications require the ability to inject external knowledge, unseen in training, into LLM inference. For instance, chatbots [1, 5, 16] and agent systems use the chatting history as supplementary knowledge to generate personalized responses; enterprise-use LLMs generate answers to queries based on their internal databases, as new knowledge, using retrieval-augmented generation (RAG) [14, 25, 27, 28, 39]; and searching engines use LLMs to read fresh and relevant web data from the internet for each user query [2, 6, 10].\n\nNot only is providing more external knowledge among the most efficient ways to boost LLM quality [7, 28, 39, 41], but the requirements of knowledge injection also become more complex and diverse. For instance, in enterprise settings, data scientists and application operators often demand the flexibility that allows them to directly specify which documents (or which parts of a document) should or should not be used as the context to answer a given query [4, 9]. Moreover, as new data is being constantly collected [3, 8], the knowledge injection method must be efficient enough to keep up with the rapidly evolving pool of knowledge.\n\nThus, the key question is *“how to design and implement a system to inject knowledge to LLMs?”*\n\nThree knowledge-injection methods exist. In-context learning and fine-tuning are the two popular options employed by knowledge-augmented generation. Fine-tuning retrains an LLM on a new dataset using a set of labeled knowledge to update all or parts of the model’s parameters. Unlike fine-tuning, in-context learning leverages the existing capabilities of the model to interpret and respond to new information, without altering the model’s parameters. Yet, compared to using a fine-tuned model, in-context learning has much higher run-time computation overhead as it has to process a much longer input (i.e., the prefill step), before the model can generate the first token.\n\nAn alternative, which we refer to as *KV-cache learning*, is to let the LLM pre-compute the KV cache¹ of the new knowledge text so that when the same knowledge is needed to supplement another LLM query, the KV cache can be directly used by LLM. This way, LLMs can directly generate the response fast as the fine-tuned model, with little extra computation overhead to prefill the text of the knowledge as in-context learning. However, KV-cache learning, with a straightforward implementation, will suffer from the large size of the KV caches.\n\nMost research in the machine-learning communities has primarily focused on the generation quality of different knowledge-injection methods, in F1 score and accuracy [15, 29, 35]. Studies have shown that both in-context learning and fine-tuning can achieve high text-generation quality if they are configured appropriately by machine-learning engineers [11, 21, 42]. However, less is known about the tradeoffs of these methods presented to system engineers who implement and maintain the LLM infrastructure for knowledge-augmented LLM applications.\n\nThis paper sheds light on these methods from a *system architecture*’s perspective (Figure 1). Specifically, we make two key arguments.\n* *Existing knowledge-injection methods–in-context learning, fine-tuning, and KV-cache learning–mainly differ in the tradeoffs between their* **modularity** *(ease of adding new knowledge and flexibility to specify injected knowledge) and* **efficiency** *(in per-query cost and response delay). (§2)*\n* *Compared to in-context learning and fine-tuning, KV-cache learning* **could** *improve both modularity and efficiency.*",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "1 Background and Motivation",
      "chunk_title": "Introduction and Key Arguments",
      "page_numbers": [
        3
      ]
    },
    {
      "id": "da2f67b1-61bb-477d-b038-67581496df16",
      "heading": "Do Large Language Models Need a Content Delivery Network? > [Preamble] > Footnote 1",
      "content": "¹KV cache is the intermediate state when LLM prefill on a text [33, 34, 40], which represents the LLM’s understanding of the knowledge.\n*The 2nd Workshop on Hot Topics in System Infrastructure (HotInfra’24), co-located with SOSP’24, November 3, 2024, Austin, TX, USA",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "[Preamble]",
      "chunk_title": "Footnote 1",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "9bb71961-99ae-4050-b65f-8484c9c3d8c8",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 1 Background and Motivation > Figure 1: Modularity vs. Efficiency",
      "content": "[Figure 1: A table showing the trade-offs between Modularity and Efficiency for In-context learning, Fine-tuning, and KV-cache learning. In-context learning has high modularity but low efficiency. Fine-tuning has low modularity but high efficiency. KV-cache learning has high modularity and high efficiency.]\n\n**Figure 1. Knowledge-injection methods make trade-offs between modularity and efficiency. With Knowledge Delivery Networks (KDNs), KV-cache learning can improve on both dimensions compared to in-context learning and fine-tuning.**\nIf a new system component, called **knowledge-delivery network (KDN)**, optimizes the management of KV caches by harnessing several emerging research efforts. (§3.2)\n\nAt a high level, a KDN serves as a backend of LLM-processed knowledge (i.e., KV caches), which can be a part of an LLM-serving system or shared across multiple LLM-serving systems. Unlike the existing LLM-serving systems [23, 26, 46], which deeply couple KV caches with LLM engines (i.e., sharing KV caches with GPUs and managing KV caches inside inferencing), KDNs call for a clean **separation** between KV-cache management and LLM serving engines, for better modularity and efficiency.\n\nWe will outline the key components of a KDN, including a storage pool of KV caches that leverages KV-cache compression, a fast KV-cache streaming system to transfer KV caches between LLM serving engines, and a KV-cache blending module that dynamically puts together multiple pieces of knowledge stored in modularized KV caches. Using a prototype of KDN, we will show that some emerging research efforts have already provided preliminary techniques, which together can make highly efficient KDNs a reality.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "1 Background and Motivation",
      "chunk_title": "Figure 1: Modularity vs. Efficiency",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "52e2c515-b8ab-40e8-873c-aa6bf3a9f640",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 2 LLM Knowledge-Injection From A System Perspective > Introduction",
      "content": "**2 LLM Knowledge-Injection From A System Perspective**\n\nKnowledge-augmented generation, particularly, fine-tuning and in-context learning, is well-studied in the literature. Fine-tuning embeds a corpus of text in the LLM’s weights, so the fine-tuned model can directly respond to a user query with a low response delay. However, as the entire corpus of text must be embedded in the model together, fine-tuning lacks the flexibility to add new knowledge and specify what knowledge should or should not be used.\n\nIn-context learning is the opposite of fine-tuning, as it allows the operators to specify which external knowledge should be used easily by putting the texts in the LLM’s input. However, the compute overhead of prefilling will grow superlinearly with the input length, causing a long response delay when more external data is added to the input.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "2 LLM Knowledge-Injection From A System Perspective",
      "chunk_title": "Introduction",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "12244f4b-1b7d-4546-a0ff-2251840f7c49",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 2 LLM Knowledge-Injection From A System Perspective > 2.1 The efficiency-modularity tradeoff",
      "content": "**2.1 The efficiency-modularity tradeoff**\n\nBoth methods can achieve similar text-generation quality if used in the right way [11, 21, 42]. Instead of viewing these options only in terms of accuracy (i.e., through the ML perspective), we compare the knowledge-injection methods along the following two system-centric metrics: **modularity** and **efficiency**.\n\n**Modularity:** In the context of knowledge-augmented LLM, the modularity of a method includes two aspects. First, a modular approach should allow service providers to specify which knowledge to use and compose them easily. Second, the overhead (e.g., time, cost) of injecting new knowledge into the LLM should be minimized.\n\nIn-context learning puts new knowledge in the model’s input, rather than the model itself. The separation of knowledge and model serves as the key to modularity—LLM service providers can specify which knowledge to use and easily compose different pieces of knowledge, which helps the LLM to avoid conflicting knowledge and improve the generation quality. In contrast, fine-tuning has poor modularity. Users cannot specify which knowledge in the fine-tuned model would be used to generate the answer. Moreover, fine-tuning needs to happen for every new knowledge and model, which may take hours to days to complete.\n\n**Efficiency:** On the other hand, the efficiency of a knowledge-augmented LLM system is measured by per-query cost and response delay during LLM inference. Cost is the computation needed for the LLM to handle a request, and response delay is defined as the time between the LLM serving system receiving a request and the generation of the first token. Viewed through this lens, in-context learning is not ideal because when using in-context learning, LLMs must spend a long time prefilling input text with the knowledge before generating the first token. In contrast, fine-tuning is better in terms of efficiency. Because the knowledge is embedded in the model’s weights, the fine-tuned model can skip the long prefill.\n\nIn short, in-context learning is more modular but sacrifices efficiency, while fine-tuning, though achieving better efficiency, suffers from the overhead of incorporating and controlling the knowledge due to poor modularity.\n**2.2 KV-Cache Learning**\n\nAlternatively, KV-cache learning stores the knowledge in a KV-generated KV cache, and injects knowledge by feeding the KV cache to the LLM, without modifying the model. A KV cache stores the knowledge in the form of the attention state generated by the model after it processes the text, so the KV cache, once generated, can be reused by the LLM to skip prefilling if the subsequent requests use the same context. When many queries reuse the same context, reusing its KV cache could reduce the per-query delay and compute usage, while still preserving the modularity as in-context learning. The idea of KV-cache learning has gained increasing attention in LLM services (e.g., [7, 36, 44]).\n\n**Why KV-cache learning pays off?** On the surface, the use of KV caches may seem merely a space-time tradeoff (trading KV cache storage space for shorter prefill), but the tradeoff is favorable for two reasons:",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "2 LLM Knowledge-Injection From A System Perspective",
      "chunk_title": "2.1 The efficiency-modularity tradeoff",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "3f674a08-8818-4354-ae92-16c5fd1c4e76",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 3 Knowledge Delivery Networks for Efficient Knowledge Injection > Figure 2: Architecture of a Knowledge Delivery Network (KDN)",
      "content": "[Figure 2: A diagram with three parts. (a) shows injecting knowledge via in-context learning, where chat history, books, news, and video data are fed as raw text to LLM serving engines. (b) shows injecting knowledge via Knowledge-Delivery Networks, where the same data sources are fed to a KDN, which then feeds KV caches to the LLM serving engines. (c) shows the high-level structure of the KDN, which includes a KV Cache Store, a KV Cache Delivery module, and a KV Cache Blend module.]\n\n**Figure 2. Architecture of a Knowledge Delivery Network (KDN).**\n* *KV caches are reused a lot.* Many contexts, especially long contexts, are frequently reused for different queries and different users. This can be viewed as the LLM version of the Pareto’s rule: an LLM, like human, uses 20% of the knowledge for 80% of the time, which means knowledge is frequently reused. Just consider that if a user asks the LLM to read a book, it is unlikely that they will only ask one book-related question.\n* *The size of KV caches increases slower than prefill delay.* As the context increases, the KV cache size grows linearly, whereas the prefill delay grows superlinearly. And, as the LLM gets bigger, more compute will happen at the feedforward layers which do not affect the KV cache size.\nmight degrade, lowering the inference quality. Thus, when the KV caches are reused by more queries repeatedly, the degraded quality will also affect more queries.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "3 Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Figure 2: Architecture of a Knowledge Delivery Network (KDN)",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "e08f44b4-06d5-4db0-8931-fd3e4007c41c",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 3 Knowledge Delivery Networks for Efficient Knowledge Injection > Limitations of Existing KV-Cache Systems",
      "content": "**3 Knowledge Delivery Networks for Efficient Knowledge Injection**\n\nLimitations of Existing KV-Cache Systems: Despite the promise, existing LLM serving systems still have some technical limitations.\n* *Limited storage for KV caches:* Currently, many serving engines only use the fast CPU/GPU memory, easily accessible by an individual LLM instance, to store KV caches. Such local memory only storage greatly limits the amount of KV caches that are stored and reused. For instance, a CPU memory of 64 GB can only store the KV cache of 160k tokens (two-half tensors) for a small-size LLM (Llama-34B). However, expanding the storage of KV caches to disk or remote servers would significantly constrain the bandwidth for loading the KV caches into GPU memory.\n* *Prefix-only reuse:* To reuse the KV cache, most systems require that the text of the KV cache must be the *prefix* of the LLM input. Even though reusing the KV cache of the prefix is naturally supported by LLMs, this assumption of “sharing prefix only” severely limits its use cases. For instance, retrieval-augmented generation (RAG) concatenates multiple retrieved text chunks in the LLM input, so most retrieved text will not be the prefix.\n* *Degraded quality with long contexts:* Finally, as more texts are added to the input as LLM’s context (i.e., long context), the LLM’s capability to capture important information",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "3 Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Limitations of Existing KV-Cache Systems",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "4978b2cb-3ca7-4f30-9b21-b97af1798d5c",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 3 Knowledge Delivery Networks for Efficient Knowledge Injection > 3.1 Knowledge Delivery Architecture",
      "content": "**3.1 Knowledge Delivery Architecture**\n\nAt first glance, these challenges facing prior KV-cache-based systems may look disparate. Yet, our insight is that they share a common need—***a separate KV-cache management system, which dynamically compresses, composes, and modifies KV caches to optimize the storage and delivery of KV caches and the LLM inference based on KV caches.*** We refer to such a system as a **Knowledge Delivery Network (KDN)**. As depicted in Figure 2, the envisioned architecture of a KDN consists of three main modules:\n* The **storage** module stores the KV caches keyed by various texts and offline modifies the KV cache content such that the inference quality will be improved when the KV caches are fed to the LLM.\n* The **delivery** module transmits the compressed KV caches from the storage device to the server running the LLM and decompresses them in GPU to be used by the LLM serving engine.\n* The **blending** module dynamically composes multiple KV caches corresponding to different texts when these texts are put together in the context of an LLM query.\n\nThe existing LLM serving systems [23, 26, 46] deeply couple KV caches with LLM engines. In contrast, the concept of KDN is to **separate** the management of KV caches and the LLM serving engines. Such separation will enable innovations on the storage, sharing, and use of KV caches, without needing to be deeply coupled with the fast-evolving LLM serving engines. By implementing these optimizations separately, a KDN serves as a critical new system module for LLM-serving ecosystems. It enables the decoupling of KV-cache management from LLM serving engines, which allows LLM serving engines to focus on fast query-driven inference, while the KDN can focus on the KV-cache-related optimizations independently.\n**3.2 Technical Roadmap**\n\nThe architecture of KDN itself does not directly address the challenges associated with KV caches; it merely breaks a",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "3 Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "3.1 Knowledge Delivery Architecture",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "43a8f5cd-9d20-43cd-93a2-87980bae44a4",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 3 Knowledge Delivery Networks for Efficient Knowledge Injection > Table 1: Comparison of different knowledge-injection methods",
      "content": "[Table 1]\n| | Modularity | Efficiency |\n| :--- | :--- | :--- | :--- |\n| | Time to inject new knowledge | Inference cost ($) per request | Response delay (s) per request |\n| Fine-tuning | 10 hours | 0.0022 | 0.21 |\n| In-context learning | 0 | 0.0149 | 10.91 |\n| KV-cache learning w/. KDN | 0.25 hours | 0.0059 | 2.97 |\n\n**Table 1. Comparison between different knowledge-injection methods under a RAG setup. With KDN, KV-cache learning is 40× faster when incorporating new knowledge than fine-tuning, and achieves 3.7× faster and 2.5× cheaper during inference compared to in-context learning.**\npotential solution into three modules. Fortunately, we observe that emerging research efforts could lead to a sufficient design for each module, thus making KDN practical.\n\n**KV-cache delivery:** The recent KV cache compression techniques make it possible to cheaply store and quickly load KV caches outside GPU and CPU memory. For instance, Cachegen [33] compresses the KV cache by quantizing and then encoding it into binary strings. LLMLingua [24] introduces a smaller language model to identify and remove non-essential tokens in the knowledge’s text, thus reducing the size of the corresponding KV cache. H2O [43] directly removes elements in the KV cache based on their importance calculated during the inference. By combining the above techniques, the memory footprint of the KV cache can be reduced by over 10×, drastically improving the loading speed and the storage cost of the KV cache.\n\n**KV-cache blending:** Some recent works also improve the composability of the KV caches. CacheBlend [45], for instance, enables arbitrarily composing different KV caches by recomputing the cross-attention between KV caches, where the recomputation only needs 10% computation of prefilling the full text. PromptCache [19] lets users define a prompt template with different segments, which allows each segment’s KV cache to be reused at different positions rather than prefix-only.\n\n**Offline KV-cache editing:** By separating KV-cache management from the LLM-serving engines, KDNs open up new possibilities to improve inference quality. Recent works have shown that if the attention matrix of an LLM input is appropriately modified, the inference quality can significantly improve [13, 42]. In other words, when a KV cache is “decoded” to KDN, KDN not only stores it but also can actively influence the LLM inference quality by offline editing the KV cache and returning the edited KV cache when it is retrieved next time, all of which is done without any change to the model itself or the input prompt.\n\n**Interfacing with LLM serving engines:** Currently, most LLM serving engines, such as vLLM [23], Huggingface TGI [23], and SGLang [46], do not readily support the injection of external provided KV caches as a part of the LLM input. Instead, their internal implementation for KV-cache management (e.g., paged memory in vLLM) is deeply cobbled with the model inference implementation. One way to deploy KDN is to augment each LLM engine with a KDN as submodule of the LLM engine. However, as elaborated in §3.1, developing a performant KDN is a substantial undertaking, so it will save much redundant engineering effort if each LLM engine maintains and evolves its KDNs. To avoid reinventing the wheel, the LLM serving engines could interface with a separate, shared KDN provider via two APIs: (i) the LLM stores the KV cache with the associated text to KDN, and (ii) the LLM retrieves the KV cache from KDN using some text. Exposing these APIs is feasible, given most popular LLM engines are open-source. Still, several design questions remain: how to leverage heterogeneous links, such as NVLink, RDMA, or PCIe, to transfer KV caches? Can the APIs be shared with other LLM functions, like disaggregated prefilling?\n\n**Early promise:** Based on these techniques, we implemented LMCache (https://github.com/LMCache/LMCache), a prototype of KDNs. We compare the modularity and efficiency of KV-cache learning with KDN to fine-tuning and in-context learning, under a RAG use case with the following setup:\n* Total size of knowledge (in text): 2 million tokens.\n* Each request: 8k tokens knowledge plus 2k tokens user chatting history.\n* Language model: Llama 3-L-70B [18].\n* Hardware: 2 Nvidia A40 GPUs.\n\nModularity is measured by the time² of injecting the knowledge base into LLM, and efficiency is measured by inference cost³ and response delay per request. Table 1 shows that with the KDN, KV-cache learning can be 40× faster than fine-tuning when injecting new knowledge, and it also achieves 3.7× cheaper and 2.5× faster during inference time compared to in-context learning.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "3 Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Table 1: Comparison of different knowledge-injection methods",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "43a12f74-905d-4f21-8603-7695b857f3aa",
      "heading": "Do Large Language Models Need a Content Delivery Network? > 4 Conclusion > Conclusion",
      "content": "**4 Conclusion**\n\nIn short, this paper makes a case for (1) the separation between the management of knowledge in the form of KV caches, and LLM serving engines, and (2) a Knowledge-Delivery Network (KDN) as a new LLM system component that integrates recent research developments to optimize the efficiency (in speed and cost) of KV-cache-based knowledge injection. We hope this paper can inspire more research to tackle the aforementioned problems.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "4 Conclusion",
      "chunk_title": "Conclusion",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "c431974f-5fc3-4062-8141-317cf5928017",
      "heading": "Do Large Language Models Need a Content Delivery Network? > [Postamble] > Footnotes",
      "content": "²We assume the chatting history cannot be pre-learned by fine-tuning. The time for fine-tuning the model is estimated from Llama-Adapter [15].\n³The inference cost is calculated based on the AWS cloud price [12].",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "[Postamble]",
      "chunk_title": "Footnotes",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "ee5f6643-f180-4847-aaaf-33304626ccbd",
      "heading": "Do Large Language Models Need a Content Delivery Network? > References > Full List of References",
      "content": "References\n\n[1] character.ai. (Accessed on 09/07/2024).\n[2] Enterprise search: an llm-enabled out-of-the-box search engine. https://io.google/2023/program/27cce05f-df4c-4ab2-9a59-5b46b6daef0f/. (Accessed on 09/07/2024).\n[3] How many websites are there in the world? (2024) – siteefy. https://siteefy.com/how-many-websites-are-there/. (Accessed on 09/08/2024).\n[4] Instruction to exclude certain information when generating answer - openai developer forum. https://community.openai.com/t/instruction-to-exclude-certain-information-when-generating-answer/470451. (Accessed on 09/08/2024).\n[5] Introducing the chat api. https://openai.com/index/chatgpt/. (Accessed on 09/07/2024).\n[6] Perplexity. https://www.perplexity.ai/. (Accessed on 09/07/2024).\n[7] Perplexity. https://pplx-api.github.io/2024-09-17-release/. (Accessed on 09/14/2024).\n[8] Perplexity partners with elevenlabs to launch ‘discover daily’ podcast. https://www.perplexity.ai/hub/blog/perplexity-partners-with-elevenlabs-to-launch-discover-daily-podcast. (Accessed on 09/08/2024).\n[9] Revolutionizing operational efficiency: Unifying analytics and observability for seamless decision-making. https://www.conivix.com/blog/revolutionizing-operational-efficiency-unifying-analytics-and-observability-for-seamless-decision-making/. (Accessed on 09/07/2024).\n[10] App search: Ai search engine for search. https://consensus.app/search/. (Accessed on 09/07/2024).\n[11] Simone Aghisha, Massimo Rizzoli, Gabriele Svelto, and Seyed-Mohsen Moosavi-Dezfooli. Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue. arXiv preprint arXiv:2404.08811, 2024.\n[12] Amazon Web Services. Ec2 on-demand instance pricing – amazon web services, 2024. (Accessed on November 02, 2024).\n[13] Anonymous. Model full where need to attend: Faithfulness meets automatic attention steering. In Submitted to ACL Rolling Review - June 2024. ACL, Under review.\n[14] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 17754–17762, 2023.\n[15] Yujia Cui, Zhipeng Zhang, Zheng Sha, George Karypis, and He He. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814, 2021.\n[16] Panos Davalos, Yiming Zheng, Sheng Shen, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, and Ion Stoica. Can large language models explore the evaluating limits in human preference. arXiv preprint arXiv:2403.04132, 2024.\n[17] Hongxia Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2023.\n[18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Ahmed, Akhil Mathur, Alok Schiller, Tony Tung, and Anshul Tuteja. The illusion of ai models. arXiv preprint arXiv:2407.17823, 2024.\n[19] In Finer, Simran Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khan, Delwal, and Lin Zhang. Prompt cache: Modular attention reuse for low-latency inference, 2023.\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n[21] Anura Gupta, Anup Shirgaonkar, Anup de Luis Balaguer, Bruno Silva, Ankur Holani, Ankur Jain, Jennifer Lee, O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. arXiv preprint arXiv:2401.08406, 2024.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[23] huggingface. text-generation-inference, 2024.\n[24] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023.\n[25] Zhenghao Liu, Faezeh Fath, Jiacheng Liu, Zichang Liu, De-An Huang, Wujun Zhou, Nan Jiang, Yinan Cao, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023.\n[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474, 2020.\n[27] Haoran Li, Yiran, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge bases. arXiv preprint arXiv:2401.10446, 2024.\n[28] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Staying on topic with k-means: fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1194–1208, 2022.\n[29] Wenliang Li, Xizhou Zhu, Guangwen Li, and Gao Huang. Visual instruction tuning. Advances in neural information processing systems, 36, 2023.\n[30] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14, pages 21–37. Springer, 2016.\n[31] Ze Liu, Kai, Yutong Lin, Yue Cao, Han Hu, Yixuan Yuan, Zheng Zhang, Zhenda Xie, Hanbo Bao, Wendong Zhang, Deblin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Yuan, Zheng Zhang, Zhenda Xie, Hanbo Bao, Wendong Zhang, Deblin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\n[33] Ze Liu, Hanbo Bao, Yue Cao, Yixuan Yuan, Jianfeng Gao, Han Hu, Chunyuan Li, and Jianxin Wu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3202–3211, 2022.\n[34] Ze Liu, Yixuan Yuan, Yue Cao, Zheng Zhang, Zhenda Xie, Hanbo Bao, Geng, Chunyuan Li, and Jianfeng Gao. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12009–12019, 2022.\n[35] Ze Liu, Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Yue Cao, Han Hu, and Yixuan Yuan. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\n[36] Ze Liu, Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Yue Cao, Han Hu, and Yixuan Yuan. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021.\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10781–10789, 2019.\n[39] Chengcheng Wang, Yueze Liu, Yuanhan Yue, Xiangyu Tang, Hanbang Zhang, Zhaoxiang Jiang, Yuxin Yao, Wenyang Gao, Xuming Hu, Zehan Li, et al. A Survey on Parallelity in large language models: From model, retrieval and domain-specificity. arXiv preprint arXiv:2310.07521, 2023.\n[40] Jinyi Su, Xiaochen Li, Shian Liu, Shiliang Sun, Yifan Zheng, Chengcheng Wang, Kunta Lo, Shan Lin, and Jianchen Lv. Cache-tuned: Fast large language model serving with cached knowledge fusion. arXiv preprint arXiv:2407.17823, 2024.\n[41] Zhenrui Yue, Honglei Zhuang, Ajian Bai, Kai Hui, Rolf Jagerman, Haiao Song, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Wenderski. Efficient scaling of llms for retrieval augmented generation. arXiv preprint arXiv:2410.04343, 2024.\n[42] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. Tell your model where to attend: Post-hoc attention steering for llms. arXiv preprint arXiv:2310.02538, 2023.\n[43] Zhenrui Zhang, Zhaoning Ruan, Chao Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongzheng Li, and Xi Qiao. Llama-adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. arXiv preprint arXiv:2303.16199, 2023.\n[44] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Loft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131, 2024.\n[45] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\n[46] Lianmin Zheng, Ying Sheng, Tianle Cai, Max G. Levy, Anastasia Razdaibiedina, Shiyi Cao, Dacheng Li, and Ion Stoica. Lmsys-chat-1m: A large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998, 2023.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "References",
      "chunk_title": "Full List of References",
      "page_numbers": [
        5
      ]
    }
  ]
}