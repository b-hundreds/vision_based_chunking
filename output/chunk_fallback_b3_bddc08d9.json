{
  "id": "fallback_b3_bddc08d9",
  "content": "Lost in the Middle: How Language Models Use Long Contexts > References Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv:2302.00083. Ohad Rubin and Jonathan Berant. 2023. Long-range language modeling with self-retrieval. arXiv:2306.13421. Chinnadhurai Sankar, Sandeep Subramanian, Chris Pal, Sarath Chandar, and Yoshua Bengio. 2019. Do neural dialog systems use the conversation history effectively? an empirical study. In Proc. of ACL. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-rant, and Omer Levy. 2023. ZeroSCROLLS: A zero-shot benchmark for long text understanding. arXiv:2305.14196. Vatsal Sharan, Sham Kakade, Percy Liang, and Gregory Valiant. 2018. Prediction with a short memory. In Proc. of STOC. Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-joon Seo, Rich James, Mike Lewis, Luke Zettle-moyer, and Wen tau Yih. 2023. REPLUG: Retrieval-augmented black-box language mod-els. arXiv:2301.12652. Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Wishal Arora, Joshua Lane, Merve Becer, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 2022. BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv:2208.03188. Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? In Proc. of EMNLP. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UL2: Unifying language learning paradigms. arXiv:2205.05131. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Mor-ris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vin-odkumar Prabhakaran, Mark Diaz, Ben Hutchin-son, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera, Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. LaMDA: Language models for dialog applications. arXiv:2201.08239. Hugo Touvron, Thibaut Lavril, Gautier Izac-ard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. LLaMA: Open and efficient foundation language models. arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Pe-ter Albert, Amjad Almahairi, Yasmine Bazi, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernan-des, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-thia Gao, Vedanuj Goswami, Naman Goyal, An-thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-ren, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin True </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > References Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-gela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. At-tention is all you need. In Proc. of NeurIPS. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-former: Self-attention with linear complexity. arXiv:2006.04768. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for longer sequences. In Proc. of NeurIPS. True </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > A Ambiguity in Multi-Document QA Distractor Documents A Ambiguity in Multi-Document QA Distractor Documents Following past work on NaturalQuestions-Open (Izacard et al., 2021; Izacard and Grave, 2021, inter alia), we use a Wikipedia dump from late 2018 as our retrieval corpus. However, this standard Wikipedia dump has a small amount of temporal mismatch with the NaturalQuestions annotations. For example, consider the question \u201cwhat nfl team does robert griffin iii play for\u201d. The Natu-ralQuestions annotated answer is \u201ccurrently a free agent\u201d. However, the Wikipedia retrieval corpus contains the information that he plays for the \u201cBalti-more Ravens\u201d, since he was released from the team between the Wikipedia dump\u2019s timestamp and the NaturalQuestions annotation process. We use the ambiguity annotations of Min et al. (2020) to create a subset unambiguous questions. Experiments on this unambiguous subset of the data show similar results and conclusions as the experiments on the full questions collection (Figure 12). False </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > A Ambiguity in Multi-Document QA Distractor Documents <image> A line graph titled \"20 Total Retrieved Documents (~4k tokens, unambiguous questions)\" with the x-axis \"Position of Document with the Answer\" (1st, 5th, 10th, 15th, 20th) and the y-axis \"Accuracy\" (from 60 to 75). Six lines are plotted for different models: claude-1.3, claude-1.3-100k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, mpt-30b-instruct, and longchat-13b-16k. All models show a U-shaped performance curve, with higher accuracy at the beginning and end positions and a dip in the middle. </image> Figure 12: Language model performance on a unambiguous subset of questions. True</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > B Random Distractors in Multi-Document QA B Random Distractors in Multi-Document QA We also run multi-document question answering experiments with random Wikipedia documents as distractors, which allows us to ablate the impact of retrieved distractors (hard negatives). Note that in this setting, the the document containing the answer can often be identified with simple heuristics (e.g., lexical overlap with the query). Figure 13 presents the results of this experiment. Although all models have higher absolute accuracy in this setting, they surprisingly still struggle to reason over their entire input context, indicating that their performance degradation is not solely due to an inability to identify relevant documents. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > C Randomizing Distractor Order in Multi-Document QA C Randomizing Distractor Order in Multi-Document QA Our prompt instructs the language model to use the provided search results to answer the question. There may be a prior in the pre-training or instruction fine-tuning data to treat search results as sorted by decreasing relevance (i.e., the documents near the beginning of the input context are more likely to be useful than those at the end). To validate that our conclusions are not simply a byproduct of this bias, we run experiments with the modified instruction \u201cWrite a high-quality answer for the given question using only the provided search results (some of which might be irrelevant). The search results are ordered randomly.\u201d In addition, we randomly shuffle the k \u2212 1 distractor documents. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > B Random Distractors in Multi-Document QA <image> A line graph titled \"20 Total Retrieved Documents (~4k tokens, random distractors)\" with the x-axis \"Position of Document with the Answer\" (1st, 5th, 10th, 15th, 20th) and the y-axis \"Accuracy\" (from 65 to 80). Six lines are plotted for different models: claude-1.3, claude-1.3-100k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, mpt-30b-instruct, and longchat-13b-16k. All models show a U-shaped performance curve, with higher accuracy at the beginning and end positions and a dip in the middle. </image> Figure 13: Language model performance on multi-document QA when using random distractors, rather than retrieved distractors. True</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > C Randomizing Distractor Order in Multi-Document QA Figure 14 presents the results of this experiment. We continue to see a U-shaped performance curve, with performance degrading when language models must use information in the middle of their input contexts. Comparing the results in \u00a72.3 with those when randomizing the distractor order and mentioning such in the prompt, we see that randomization slightly decreases performance when the relevant information is at the very beginning of the context, and slightly increases performance when using information in the middle and end of the context. <image> A line graph titled \"20 Total Retrieved Documents (~4k tokens, randomly ordered)\" with the x-axis \"Position of Document with the Answer\" (1st, 5th, 10th, 15th, 20th) and the y-axis \"Accuracy\" (from 55 to 75). Six lines are plotted for different models: claude-1.3, claude-1.3-100k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, mpt-30b-instruct, and longchat-13b-16k. All models show a U-shaped performance curve. </image> Figure 14: Language model performance when randomizing the order of the distractors, rather than presenting them in order of decreasing relevance) and mentioning as such in the prompt. True</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > D GPT-4 Performance D GPT-4 Performance We evaluate GPT-4 (8K) on a subset of 500 random multi-document QA examples with 20 total documents as in-context (Figure 15). GPT-4 achieves higher absolute performance than any other language model, but still shows a U-shaped performance curve\u2014its performance is highest when relevant information occurs at the very start or end of the context, and performance degrades when it must use information in the middle of its input context. <image> A line graph titled \"20 Total Retrieved Documents (~4k tokens, 500 question sample)\" with the x-axis \"Position of Document with the Answer\" (1st, 5th, 10th, 15th, 20th) and the y-axis \"Accuracy\" (from 50 to 90). Six lines are plotted for different models: claude-1.3, claude-1.3-100k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, mpt-30b-instruct, and gpt-4-0613. The gpt-4-0613 line is consistently higher than the others, but still exhibits a U-shaped curve, starting around 85%, dipping to around 78%, and rising back to around 85%. The other models show similar U-shaped curves at lower accuracy levels. </image> Figure 15: Although GPT-4 has higher absolute performance than other models, its performance still degrades when relevant information occurs in the middle of the input context. False</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > E Llama-2 Performance E Llama-2 Performance We evaluate Llama-2 (Touvron et al., 2023b) on multi-document QA with 20 total documents in each input context. The Llama tokenizer produces longer sequences than the tokenizers for our previously-studied models, so we discard 20 exam- True</CONTINUES] </CHUNK> Lost in the Middle: How Language Models Use Long Contexts > Appendix > E Llama-2 Performance ples (out of 2655) that exceed Llama-2\u2019s maximum context length of 4096 tokens. We experiment with models of varying sizes (7B, 13B, and 70B parameters), with and without additional supervised fine-tuning and reinforcement learning from human feedback (\u201c-chat-\u201d models). The results are presented in Figure 16. Comparing Llama-2 models of varying sizes, we find that only the larger models (13B and 70B) exhibit the U-shaped performance curve (i.e., both primacy and recency bias)\u2014the smallest Llama-2 models (7B) are solely recency-biased. Given these results, we hypothesize that prior work (e.g., Khandelwal et al., 2018; Sun et al., 2021) did not previously observe any primacy bias in language models because the models they studied were too small (less than 1B parameters). Comparing between Llama-2 models with and without additional supervised fine-tuning and reinforcement learning from human feedback, we see that additional fine-tuning dramatically improves performance on the multi-document QA task. The 7B models with and without additional fine-tuning show minimal primacy bias, and are largely recency-biased. The 13B base model has a dramatic primacy and recency bias\u2014there is a 20-point accuracy disparity between the best- and worst-case performance. Applying additional fine-tuning to the 13B seems to slightly reduce this bias (10-point worst-case degradation), but the bias remains significant. However, the 70B models with and without additional fine-tuning have largely similar trends (showing both primacy and recency bias), and additional fine-tuning minimally changes the positional bias severity. <image> A line graph titled \"20 Total Retrieved Documents (~4K tokens)\" with the x-axis \"Position of Document with the Answer\" (1st, 5th, 10th, 15th, 20th) and the y-axis \"Accuracy\" (from 20 to 70). Six lines are plotted for different Llama-2 models: Llama-2-7b-chat-hf, Llama-2-13b-chat-hf, Llama-2-70b-chat-hf, Llama-2-7b-hf, Llama-2-13b-hf, and Llama-2-70b-hf. The 7B models show a recency bias (rising trend). The 13B and 70B models show a U-shaped performance curve. </image> Figure 16: Multi-document QA performance (20 total documents) of Llama-2 models of varying sizes (7B, 13B, 70B parameters), with and without additional supervised fine-tuning and reinforcement learning from human feedback (\u201c-chat-\u201d models). True</CONTINUES] </CHUNK>",
  "heading_hierarchy": [
    "Fallback Content"
  ],
  "page_numbers": [
    13,
    14,
    15,
    16
  ],
  "continuation_flag": "False",
  "source_batch": 3,
  "metadata": {
    "is_fallback": true,
    "raw_length": 15873
  }
}