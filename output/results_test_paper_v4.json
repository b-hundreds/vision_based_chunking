{
  "document_name": "test_paper.pdf",
  "total_pages": 6,
  "chunks": [
    {
      "id": "b994ebfb-01e1-425c-88f7-c9d25f9fb69a",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Title and Authors > Title and Authors",
      "content": "Do Large Language Models Need a Content Delivery Network?\n\nYihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang\nThe University of Chicago",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Title and Authors",
      "chunk_title": "Title and Authors",
      "page_numbers": [
        1
      ]
    },
    {
      "id": "be050c4f-e8e7-4077-83b8-4d9acdfd9302",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Abstract > Abstract",
      "content": "As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling modular and efficient injection of new knowledge in LLM inference is critical. We argue that compared to the more popular fine-tuning and in-context learning, using KV caches as the medium of knowledge could simultaneously improve the modularity of knowledge injection and the efficiency of LLM serving with low cost and fast response. To make it practical, we envision a Knowledge Delivery Network (KDN), a new component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. Just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. An open-sourced KDN prototype: https://github.com/LMCache/LMCache.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Abstract",
      "chunk_title": "Abstract",
      "page_numbers": [
        2
      ]
    },
    {
      "id": "d698f864-44f3-47a1-9231-db39c90a68d0",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > Introduction to Knowledge Injection",
      "content": "**1 Background and Motivation**\n\nTraditionally, machine learning models, such as computer vision [22, 30, 31, 38] and image generation [20, 32, 37], learn all the knowledge from the training data. However, with the rapid usage growth of large language models (LLMs), applications require the ability to inject external knowledge, unseen in training, into LLM inference. For instance, chatbots [1, 5, 16] and agent systems use the chatting history as supplementary knowledge to generate personalized responses; enterprise-use LLMs ingest or answer queries based on their internal databases, as new knowledge, using retrieval-augmented generation (RAG) [14, 25, 27, 35, 39] and searching engines use LLMs to read fresh and relevant web data from the internet for each user query [2, 6, 10].\n\nNot only is providing more external knowledge among the most efficient ways to boost LLM quality [17, 25, 39, 41], but the requirements of knowledge injection also become more complex and diverse. For instance, in enterprise settings, data scientists and application operators often demand the flexibility that allows them to directly specify which documents (or which parts of a document) should or should not be used as the context to answer a given query [4, 9]. Moreover, as new data is being constantly collected [3, 8], the knowledge injection method must be efficient enough to keep up with the rapidly evolving pool of knowledge.\nThus, the key question is “how to design and implement a system to inject knowledge to LLMs?”\n\nThree knowledge-injection methods exist. In-context learning and fine-tuning are the two popular options employed by knowledge-augmented generation. Fine-tuning retrains an LLM on a new dataset using a set of labeled knowledge to update all or parts of the model’s parameters. Unlike fine-tuning, in-context learning leverages the existing capabilities of the model to interpret and respond to new information, without altering the model’s parameters. Yet, compared to using a fine-tuned model, in-context learning has much higher run-time computation overhead as it has to process a much longer input (i.e., the prefill step), before the model can generate the first token.\n\nAn alternative, which we refer to as KV-cache learning, is to let the LLM pre-compute the KV cache¹ of the new knowledge text so that when the same knowledge is needed to supplement another LLM query, the KV cache can be directly used by LLM. This way, LLMs can directly generate the response fast as the fine-tuned model, with little extra computation overhead to prefill the text of the knowledge as in-context learning. However, KV-cache learning, with a straightforward implementation, will suffer from the large size of the KV caches.\n\nMost research in the machine-learning communities has primarily focused on the generation quality of different knowledge-injection methods, in F1 score and accuracy [15, 28, 35]. Studies have shown that both in-context learning and fine-tuning can achieve high text generation quality if they are configured appropriately by machine-learning engineers [11, 21, 42]. However, less is known about the tradeoffs of these methods presented to system engineers who implement and maintain the LLM infrastructure for knowledge-augmented LLM applications.\n\nThis paper sheds light on these methods from a *system architecture* perspective (Figure 1). Specifically, we make two key arguments.\n* *Existing knowledge-injection methods–in-context learning, fine-tuning, and KV-cache learning–mainly differ in the tradeoffs between their* **modularity** *(ease of adding new knowledge and flexibility to specify injected knowledge) and* **efficiency** *(in per-query cost and response delay).* **(§2)**\n* *Compared to in-context learning and fine-tuning, KV-cache learning* **could** *improve both modularity and efficiency.*",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "Introduction to Knowledge Injection",
      "page_numbers": [
        3,
        4
      ]
    },
    {
      "id": "5c315217-6454-4949-947f-9bfc47074ac1",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > Footnotes",
      "content": "The 2nd Workshop on Hot Topics in System Infrastructure (HotInfra’24), co-located with SOSP’24, November 3, 2024, Austin, TX, USA\n\n¹KV cache is the intermediate state when LLM prefill on a text [33, 34, 40], which represents the LLM’s understanding of the knowledge.\n¹Architecture refers to the interfaces between the modules, and we leave the refinement of implementation to future work.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "Footnotes",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "cd70fdc0-1682-4207-857e-e364f0321292",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > Figure 1: Modularity vs. Efficiency Tradeoffs",
      "content": "Figure 1. Knowledge-injection methods make trade-offs between modularity and efficiency. With Knowledge Delivery Networks (KDNs), KV-cache learning can improve on both dimensions compared to in-context learning and fine-tuning.\n\nThis figure presents a simple 2x2 table comparing two knowledge-injection methods, \"In-context learning\" and \"Fine-tuning,\" across two dimensions: \"Modularity\" and \"Efficiency.\"\n- **In-context learning:** Is shown to have good Modularity (indicated by a green check mark) but poor Efficiency (indicated by a red 'X' mark).\n- **Fine-tuning:** Is shown to have poor Modularity (red 'X' mark) but good Efficiency (green check mark).\nThe caption suggests that a third method, KV-cache learning with Knowledge Delivery Networks (KDNs), can achieve improvements in both dimensions, overcoming the tradeoff presented by the other two methods.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "Figure 1: Modularity vs. Efficiency Tradeoffs",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "c8ff58d8-5fd0-415e-b190-31c6103bb0a6",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Background and Motivation > Introduction to Knowledge Delivery Networks (KDN)",
      "content": "If a new system component, called **knowledge-delivery network (KDN)**, optimizes the management of KV caches by harnessing several emerging research efforts. (§3.2)\n\nAt a high level, a KDN serves as a backend of LLM-processed knowledge (i.e., KV caches), which can be a part of an LLM-serving system or shared across multiple LLM-serving systems. Unlike the existing LLM-serving systems [23, 26, 46], which deeply couple KV caches with LLM engines (i.e., sharing KV caches with GPUs and managing KV caches inside inferencing), KDNs call for a clean **separation** between KV-cache management and LLM serving engines, for better modularity and efficiency.\n\nWe will outline the key components of a KDN, including a storage pool of KV caches that leverages KV-cache compression, a fast KV-cache streaming system to transfer KV caches between LLM serving engines, and a KV-cache blend module that dynamically puts together multiple pieces of knowledge stored in modularized KV caches. Using a prototype of KDN, we will show that some emerging research efforts have already provided preliminary techniques, which together can make highly efficient KDNs a reality.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Background and Motivation",
      "chunk_title": "Introduction to Knowledge Delivery Networks (KDN)",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "fa6ba9c0-1d84-4dc1-9f77-4db5715d833e",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From a System Perspective > Overview",
      "content": "**2 LLM Knowledge-Injection From a System Perspective**\n\nKnowledge-augmented generation, particularly, fine-tuning and in-context learning, is well-studied in the literature. Fine-tuning embeds a corpus of text in the LLM’s weights, so the fine-tuned model can directly respond to a user query with a low response delay. However, as the entire corpus of texts must be embedded in the model together, fine-tuning lacks the flexibility to add new knowledge and specify what knowledge should or should not be used.\n\nIn-context learning is the opposite of fine-tuning, as it allows the operators to specify which external knowledge should be used easily by putting the texts in the LLM input. However, the compute overhead of prefilling will grow superlinearly with the input length, causing a long response delay when more external data is added to the input.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From a System Perspective",
      "chunk_title": "Overview",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "34ef55f8-4386-4eb3-9f31-45dbeb7bd59d",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From a System Perspective > The efficiency-modularity tradeoff",
      "content": "**2.1 The efficiency-modularity tradeoff**\n\nBoth methods can achieve similar text-generation quality if used in the right way [11, 21, 42]. Instead of viewing these options only in terms of accuracy (i.e., through the ML perspective), we compare the knowledge-injection methods along the following two system-centric metrics: **modularity** and **efficiency**.\n\n**Modularity:** In the context of knowledge-augmented LLM, the modularity of a method includes two aspects. First, a modular approach should allow service providers to specify which knowledge to use and compose them easily. Second, the overhead (e.g., time, cost) of injecting new knowledge into the LLM should be minimized.\nIn-context learning puts the new knowledge in the model’s input, rather than the model itself. The separation of knowledge and model serves as the key to modularity–LLM service providers can specify which knowledge to use and easily compose different pieces of knowledge, which helps the LLM to avoid conflicting knowledge and improve the generation quality. In contrast, fine-tuning has poor modularity. Users cannot specify which knowledge of the fine-tuned model would be used to generate the answer. Moreover, fine-tuning needs to happen for every new knowledge and model, which may take hours to days to complete.\n\n**Efficiency:** On the other hand, the efficiency of a knowledge-augmented LLM system is measured by per-query cost and response delay during LLM inference. Cost is the computation needed for the LLM to handle a request, and response delay is defined as the time between the LLM serving system receiving a request and the generation of the first token. Viewed through this lens, in-context learning is not ideal because when using in-context learning, LLMs must spend a long time prefilling input text with the knowledge before generating the first token. In contrast, fine-tuning is better in terms of efficiency. Because the knowledge is embedded in the model’s weights, the fine-tuned model can skip the long prefill.\nIn short, in-context learning is more modular but sacrifices efficiency, while fine-tuning, though achieving better efficiency, suffers from the overhead of incorporating and controlling the knowledge due to poor modularity.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From a System Perspective",
      "chunk_title": "The efficiency-modularity tradeoff",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "3f285aa3-ace7-4426-8b93-173277746860",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From a System Perspective > KV-Cache Learning",
      "content": "**2.2 KV-Cache Learning**\n\nAlternatively, KV-cache learning stores the knowledge in a pre-generated KV cache, and injects knowledge by feeding the KV cache to the LLM, without modifying the model. A KV cache stores the knowledge in the form of the attention state generated by the model after it processes the text, so the KV cache, once generated, can be reused by the LLM to skip prefilling if the subsequent requests use the same context. When many queries reuse the same context, reusing its KV cache could reduce the per-query delay and compute usage, while still preserving the modularity as in-context learning. The idea of KV-cache learning has gained increasing attention in LLM services (e.g., [7, 36, 44]).\n\n**Why does KV-cache learning pay off?** On the surface, the use of KV caches may seem merely a space-time tradeoff (trading KV cache storage space for shorter prefill), but the tradeoff is favorable for two reasons:\n* *KV caches are reused a lot.* Many contexts, especially long contexts, are frequently reused for different queries and different users. This can be viewed as the LLM version of the Pareto’s rule: an LLM, like human, uses 20% of the knowledge for 80% of the time, which means knowledge is frequently reused. Just consider that if a user asks the LLM to read a book, it is unlikely that they will only ask one book-related question.\n* *The size of KV caches increases slower than prefill delay.* As the context increases, the KV cache size grows linearly, whereas the prefill delay grows superlinearly. And, as the LLM gets bigger, more compute will happen at the feedforward layers which do not affect the KV cache size.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From a System Perspective",
      "chunk_title": "KV-Cache Learning",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "6e97d925-99fe-45d7-8aac-aeb1581c72e8",
      "heading": "Do Large Language Models Need a Content Delivery Network? > LLM Knowledge-Injection From a System Perspective > Figure 2: KDN Architecture",
      "content": "Figure 2. Architecture of a Knowledge Delivery Network (KDN)\n\nThis figure consists of three diagrams (a, b, c) that illustrate the evolution from traditional knowledge injection to the proposed Knowledge Delivery Network (KDN) architecture.\n\n(a) **Injecting knowledge via in-context learning (feeding text or raw data to LLMs):** This diagram shows various data sources (Chat history, Books, News, Video) being fed directly into multiple \"LLM serving engines.\" This represents the standard in-context learning approach where raw data is processed at inference time.\n\n(b) **Injecting knowledge via Knowledge-Delivery Networks (feeding KV caches to LLMs):** This diagram shows the same data sources feeding into a central \"Knowledge Delivery Network (KDN)\". The KDN then provides processed KV caches to the \"LLM serving engines.\" This illustrates the proposed separation of knowledge processing from the LLM serving.\n\n(c) **High-level structure of the proposed Knowledge Delivery Network (KDN):** This diagram details the internal components of the KDN. It shows a \"KV Cache Store\" feeding into a \"KV Cache Delivery\" module, which in turn feeds a \"KV Cache Blend\" module. The functions of these modules are described:\n- **KV Cache Store:** \"Better long-context quality via opportunistic attention steering\"\n- **KV Cache Delivery:** \"Fast KV cache delivery via encoding & streaming\"\n- **KV Cache Blend:** \"KV cache composition via selective recomposition\"\nThis diagram clarifies the KDN's role in storing, efficiently delivering, and composing knowledge in the form of KV caches.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "LLM Knowledge-Injection From a System Perspective",
      "chunk_title": "Figure 2: KDN Architecture",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "a476f4d8-5556-48ea-98c5-addd35e408d0",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge Injection > Limitations of Existing Systems",
      "content": "**3 Knowledge Delivery Networks for Efficient Knowledge Injection**\n\n**Limitations of Existing KV-Cache Systems:** Despite the promise, existing KV-caching learning systems still have some technical limitations.\n* *Limited storage for KV caches:* Currently, many serving engines only use the fast GPU memory (easily accessible by an individual LLM instance) to store KV caches. Such local-memory-only storage greatly limits the amount of KV caches that can be stored and reused. For instance, a CPU memory of 64 GB can only store the KV cache of 160k tokens (two pdf reports) for a small-size LLM (Llama-34B). However, expanding the storage of KV caches to disk or remote servers would significantly constrain the bandwidth for loading the KV caches into GPU memory.\n* *Prefix-only reuse:* To reuse the KV cache, most systems require that the text of the KV cache must be the prefix of the LLM input. Even though reusing the KV cache of the prefix is naturally supported by LLMs, this assumption of “sharing prefix only” severely limits its use cases. For instance, retrieval-augmented generation (RAG) concatenates multiple retrieved text chunks in the LLM input, so most retrieved text is not in the prefix.\n* *Degraded quality with long contexts.* Finally, as more texts are added to the input as LLM’s context (i.e., long context), the LLM’s capability to capture important information might degrade, lowering the inference quality. Thus, when the KV caches are reused by more queries repeatedly, the degraded quality will also affect more queries.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Limitations of Existing Systems",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "4563f79a-ab34-49c3-8a56-d4889f81cfa2",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge Injection > Proposed KDN Architecture",
      "content": "**3.1 Knowledge Delivery Architecture**\n\nAt first glance, these challenges facing prior KV-cache-based systems may look disparate. Yet, our insight is that they share a common need—*a separate KV-cache management system, which dynamically compresses, composes, and modifies KV caches to optimize the storage and delivery of KV caches and the LLM inference based on KV caches.* We refer to such a system as a **Knowledge Delivery Network (KDN)**. As depicted in Figure 2, the envisioned architecture of a KDN consists of three main modules:\n* The **storage** module stores the KV caches keyed by various texts and offline modifies the KV cache content such that the inference quality will be improved when the KV caches are fed to the LLM.\n* The **delivery** module transmits the compressed KV caches from the storage device to the server running the LLM and decompresses them in GPU to be used by the LLM serving engine.\n* The **blending** module dynamically composes multiple KV caches corresponding to different texts when these texts are put together in the context of an LLM query.\n\nThe existing LLM-serving systems [23, 26, 46] deeply couple KV caches with LLM engines. In contrast, the concept of KDN is to **separate** the management of KV caches and the LLM-serving engines. Such separation will enable innovations on the storage, sharing, and use of KV caches, without needing to be deeply coupled with the fast-evolving LLM serving engines. By implementing these optimizations separately, a KDN serves as a critical sub-system module for LLM-serving ecosystems. It enables the decoupling of KV-cache management from LLM serving engines, which allows LLM serving engines to focus on fast query-driven inference, while the KDN can focus on the KV-cache-related optimizations independently.\n**3.2 Technical Roadmap**\n\nThe architecture of KDN itself does not directly address the challenges associated with KV caches; it merely breaks a",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Proposed KDN Architecture",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "8b2dd243-7ff7-44c4-affb-ac47568ea7c8",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge Injection > Table 1: Method Comparison",
      "content": "Table 1. Comparison between different knowledge-injection methods under a RAG setup. With KDN, KV-cache learning is 40× faster when incorporating new knowledge than fine-tuning, and achieves 3.7× faster and 2.5× cheaper during inference compared to in-context learning.\n\nThis table compares three knowledge-injection methods across three key metrics related to modularity and efficiency.\n\n| | Modularity | Efficiency | Efficiency |\n| :--- | :--- | :--- | :--- |\n| | Time to inject new knowledge | Inference cost ($) per request | Response delay (s) per request |\n| **Fine-tuning** | 10 hours | 0.0022 | 5.01 |\n| **In-context learning** | 0 | 0.0149 | 10.91 |\n| **KV-cache learning w/ KDN** | 0.25 hours | 0.0059 | 2.97 |\n\n**Interpretation:**\nThe table provides quantitative data supporting the paper's central argument.\n- **Modularity (Time to inject new knowledge):** In-context learning is instantaneous (0 hours), while fine-tuning is very slow (10 hours). KV-cache learning with KDN offers a fast alternative (0.25 hours), making it significantly more modular than fine-tuning for incorporating new knowledge.\n- **Efficiency (Cost and Delay):** Fine-tuning is the most cost-effective per request ($0.0022) but has a moderate response delay (5.01s). In-context learning is the most expensive ($0.0149) and slowest (10.91s). KV-cache learning with KDN provides the fastest response time (2.97s) and is significantly cheaper than in-context learning ($0.0059), though more expensive than fine-tuning.\n\nThe caption highlights the key takeaways: KDN-based KV-cache learning is 40x faster for knowledge injection than fine-tuning, and during inference, it is 3.7x faster and 2.5x cheaper than in-context learning.\npotential solution into three modules. Fortunately, we observe that emerging research efforts could lead to a sufficient design for each module, thus making KDN practical.\n\n**KV-cache delivery:** The recent KV cache compression techniques make it possible to cheaply store and quickly load KV caches outside GPU and CPU memory. For instance, CacheGen [33] compresses the KV cache by quantizing and then encoding it into binary strings. LLMLingua [24] introduces a smaller language model to identify and remove non-essential tokens in the knowledge’s text, thus reducing the size of the corresponding KV cache. H2O [45] directly removes elements in the KV cache based on their importance calculated during the inference. By combining the above techniques, the memory footprint of the KV cache can be reduced by over 10×, drastically improving the loading speed and the storage cost of the KV cache.\n\n**KV-cache blending:** Some recent works also improve the composability of the KV caches. CacheBlend [43], for instance, enables arbitrarily composing different KV caches by recomputing the cross-attention between KV caches, where the recomputation only needs 10% computation of prefilling the full text. PromptCache [19] lets users define a prompt template with different segments, which allows each segment’s KV cache to be reused at different positions rather than prefix-only.\n\n**Offline KV-cache editing:** By separating KV-cache management from the LLM-serving engines, KDNs open up new possibilities to improve inference quality. Recent works have shown that if the attention matrix of an LLM input is appropriately modified, the inference quality can significantly improve [13, 42]. In other words, when a KV cache is “de-posited” to KDN, KDN not only stores it but also can actively influence the LLM inference quality by offline editing the KV cache and returning the edited KV cache when it is retrieved next time, all of which is done without any change to the model itself or the input prompt.\n\n**Interfacing with LLM serving engines:** Currently, most LLM serving engines, such as vLLM [23], Huggingface TGI [26], and SGLang [46], do not readily support the injection of external provided KV caches as part of the LLM input. Instead, their internal implementation for KV-cache management (i.e., paged memory in vLLM) is deeply cobbled with the model inference implementation. One way to deploy KDN is to augment each LLM engine with a KDN as submodule of the LLM engine. However, as elaborated in §3.1, developing a performant KDN is a substantial undertaking, so it will create much redundant engineering effort if each LLM engine maintains and evolves its KDNs. To avoid reinventing the wheel, the LLM serving engines could interface with a separate, shared KDN provider via two APIs: (i) the LLM stores the KV cache with the associated text to KDN, and (ii) the LLM retrieves the KV cache from KDN using some text. Exposing these APIs is feasible given most popular LLM engines are open-source. Still, several design questions remains: how to leverage heterogeneous links, such as NVLink, RDMA, or PCIe, to transfer KV caches? Can the APIs be shared with other LLM functions, like disaggregated prefilling?",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Table 1: Method Comparison",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "a0b8058f-9163-4eeb-8c09-b259b68888f3",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Knowledge Delivery Networks for Efficient Knowledge Injection > Early Promise",
      "content": "**Early promise:** Based on these techniques, we implemented LMCache (https://github.com/LMCache/LMCache), a prototype of KDNs. We compare the modularity and efficiency of KV-cache learning with KDN to fine-tuning and in-context learning, under a RAG use case with the following setup:\n* Total size of knowledge (in text): 2 million tokens.\n* Each request: 8k tokens knowledge plus 2k tokens user chatting history.\n* Language model: Llama 3-1 70B [18].\n* Hardware: 2 Nvidia A40 GPUs.\n\nModularity is measured by the timeª of injecting the knowledge base into LLM, and efficiency is measured by inference costᶜ and response delay per request. Table 1 shows that with the KDN, KV-cache learning can be 40× faster than fine-tuning when injecting new knowledge, and it also achieves 3.7× cheaper and 2.5× faster during inference time compared to in-context learning.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Knowledge Delivery Networks for Efficient Knowledge Injection",
      "chunk_title": "Early Promise",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "ed2d2a6c-ef75-4d82-b402-cffcc6975fd1",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Conclusion > Conclusion",
      "content": "**4 Conclusion**\n\nIn short, this paper makes a case for (1) the separation between the management of knowledge in the form of KV caches, and LLM serving engines, and (2) a Knowledge-Delivery Network (KDN) as a new LLM system component that orchestrates recent research developments to optimize the efficiency (in speed and cost) of KV-cache-based knowledge injection. We hope this paper can inspire more research to tackle the aforementioned problems.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Conclusion",
      "chunk_title": "Conclusion",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "04539a7d-3199-4a41-bb1d-fb8db1db525d",
      "heading": "Do Large Language Models Need a Content Delivery Network? > Conclusion > Footnotes",
      "content": "ªWe assume the chatting history cannot be pre-learned by fine-tuning.\nᵇThe time for fine-tuning the model is estimated from Llama-Adapter [15].\nᶜThe inference cost is calculated based on the AWS cloud price [12].",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "Conclusion",
      "chunk_title": "Footnotes",
      "page_numbers": [
        4
      ]
    },
    {
      "id": "93f6ad0f-4dd5-49b9-9c38-894c55a24e40",
      "heading": "Do Large Language Models Need a Content Delivery Network? > References > Citations 1-19",
      "content": "References\n\n[1] character.ai | personalized for every moment of your day. https://character.ai/. (Accessed on 09/07/2024).\n[2] Enterprise search: an llm-enabled out-of-the-box search engine. https://io.google/2023/program/27ce05f-df4c-4ab2-9a59-5b46b6daef0f/. (Accessed on 09/07/2024).\n[3] How many websites are there in the world? (2024) – siteefy. https://siteefy.com/how-many-websites-are-there/. (Accessed on 09/08/2024).\n[4] Instruction to exclude certain information when generating answer - opensearch documentation. https://opensearch.org/docs/latest/ml-commons-plugin/exclude-certain-information-when-generating-answer/. (Accessed on 09/08/2024).\n[5] Introducing the chatgpt api. https://openai.com/index/chatgpt/. (Accessed on 09/07/2024).\n[6] Perplexity. https://www.perplexity.ai/. (Accessed on 09/07/2024).\n[7] Perplexity. https://labs.perplexity.ai/2024-09-17-release. (Accessed on 10/14/2024).\n[8] Perplexity partners with elevenlabs to launch ‘discover daily’ podcast. https://www.perplexity.ai/hub/blog/perplexity-partners-with-elevenlabs-to-launch-discover-daily-podcast. (Accessed on 09/08/2024).\n[9] Revolutionizing operational efficiency: Analytics and-ai’s observability for seamless decision-making — sumit. aimproving llm output by combining rag and fine-tuning. https://www.summit.ai/blog/revolutionizing-operational-efficiency-analytics-and-ais-observability-for-seamless-decision-making/. (Accessed on 09/07/2024).\n[10] Github - consensus is an ai search engine for research. https://consensus.app/search/. (Accessed on 09/07/2024).\n[11] Simone Alghisi, Massimo Rizzoli, Gökçen Eraslan, and Seyed Mahdi Godsi. Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue. arXiv preprint arXiv:2404.09192, 2024.\n[12] Amazon Web Services. Ec2 on-demand instance pricing – amazon web services, 2024. (Accessed on September 02, 2024).\n[13] Anonymous. Model full-stack where ai-friend faithfulness meets automatic attention steering. In Submitted to ACL Rolling Review - June 2024. ACL, Under review.\n[14] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 17754–17762, 2023.\n[15] Yixuan Chen, Wenji Zhang, Zheng Sha, George Karypis, and He He. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814, 2021.\n[16] Panos Constantopoulos, Zheng Ying Sheng, Anastasios Nikolas An-gelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, and Ion Stoica. On the performance of retrieval-augmented evaluating limits in human preference. arXiv preprint arXiv:2403.04132, 2024.\n[17] Hongxia Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2023.\n[18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aleš Kubíček, Akhil Mathur, Alex Schiff, and Tony Wu. The art of llm programming. arXiv preprint arXiv:2407.12783, 2024.\n[19] In FaaS, Bowen Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khan, Dehwei, and Lin Zhong. Prompt cache: Modular attention reuse for low-latency inference, 2023.\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n[21] Anirudha Gupta, Anup Shirgaonkar, Anant de Luis Balaguer, Bruno Serra, Aniket Relekar, Lucas C. Queiroz, and Sudeep Ghosh. Pipelines and a case study on agriculture. arXiv preprint arXiv:2401.08406, 2024.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[23] Huggingface. text-generation-inference, 2024.\n[24] Huaxiu Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lidi Qiu. Compressive prompting for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023.\n[25] Zhenghao Liu, Chen-Yu Hsu, Jia-Fong, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023.\n[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474, 2020.\n[27] Haiyan Li, Yixuan, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge bases. arXiv preprint arXiv:2401.10446, 2024.\n[28] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Staying on topic with k-means: fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1198–1209, 2022.\n[29] Weizhiu Liu, Yiliang Lv, Chong-an Di, and Fred C. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2023.\n[30] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14, pages 21–37. Springer, 2016.\n[31] Lewin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengcong Yuan, Yu Huang, Hanchi Sun, Jianing Gao, et al. Sora: A review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024.\n[32] Yuhuai Wu, Shucheng Li, Marcelo do, and Dale Schuurmans. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 24936–24954. PMLR, 2023.\n[33] Zichang Liu, Aditya Desal, Fangshuo Liao, Weitao Wang, Victor Xie, Xiaoyun Tu, and Kenji Kawaguchi. Fast context loading and dynamic batching for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2023.\n[34] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16093, 2023.\n[35] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Wenmin Zheng, and Xilun Xu. Me-cache: Boost k-nearest-centric.\narchitecture for llm serving. arXiv preprint arXiv:2407.00079, 2024.\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\n[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.\n[40] Jinyi Hu, Xiaoyuan Yi, Wen-Hua, and Lidan Shou. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In Workshop on Efficient Systems for Foundation Models (MLSys’2023), 2023.\n[41] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, et al. {TVM}: An open deep learning compiler stack for cpus, gpus, and specialized accelerators. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 578–594, 2018.\n[42] Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. Tell your model where to attend. Post-hoc attention steering for llms. arXiv preprint arXiv:2310.02538, 2023.\n[43] Xinrui Zhang, Zening Hu, Chujie Gao, Ruoxi Chen, Yu Huang, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.\n[44] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131, 2024.\n[45] Ying Sheng, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhanggeng Wang, and Beidi Chen. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. In Workshop on Efficient Systems for Foundation Models (MLSys’2023), 2023.\n[46] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanggeng Wang, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, et al. Efficiently programming large language models using sglang. arXiv preprint arXiv:2312.07104, 2023.",
      "main_heading": "Do Large Language Models Need a Content Delivery Network?",
      "section_heading": "References",
      "chunk_title": "Citations 1-19",
      "page_numbers": [
        5,
        6
      ]
    }
  ]
}