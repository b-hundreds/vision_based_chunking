{
  "id": "b1_c1_0405675e",
  "content": "<image> Three line graphs showing model accuracy based on the position of the relevant document in the input context. The x-axis is \"Position of Document with the Answer\" and the y-axis is \"Accuracy\". The three graphs correspond to \"10 Total Retrieved Documents (~2K tokens)\", \"20 Total Retrieved Documents (~4K tokens)\", and \"30 Total Retrieved Documents (~6K tokens)\". All graphs show a U-shaped curve, where accuracy is highest when the relevant document is at the very beginning or very end of the context, and lowest when it is in the middle. Different colored lines represent different models: claude-1.3, claude-1.3-100k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, mpt-30b-instruct, and longchat-13b-16k. </image> Figure 5: The effect of changing the position of relevant information (document containing the answer) on multi-document question answering performance. Lower positions are closer to the start of the input context. Performance is highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models must reason over information in the middle of their input context.",
  "heading_hierarchy": [
    "Lost in the Middle: How Language Models Use Long Contexts",
    "2 Multi-Document Question Answering"
  ],
  "page_numbers": [
    5,
    6,
    7,
    8
  ],
  "continuation_flag": "False",
  "source_batch": 1,
  "metadata": {
    "position_in_batch": 1,
    "raw_length": 1134,
    "heading_count": 2,
    "validation_warnings": [
      "This chunk's heading appears on non-consecutive page ranges: 1, 1, 1-2, 2, 2-3, 3, 3-4, 4, 4-8"
    ]
  }
}