{
  "id": "b2_c8_788cd9e9",
  "content": "domain QA results. We see that reader model performance saturates long before retriever performance saturates, indicating that readers are not effectively using the extra context. Using more than 20 retrieved documents only marginally improves reader performance (< 1.5% for GPT-3.5-Turbo and ~1% for Claude-1.3), while significantly increasing the input context length (and thus latency and cost). These results, coupled with the observation that models are often better at retrieving and using information at the start or end of the input contexts, suggest that effective re-ranking of retrieved documents (pushing relevant information closer to the start of the input context) or ranked list truncation (retrieving fewer documents when appropriate; Arampatzis et al., 2009) may be promising directions for improving how language-model-based readers use retrieved context.",
  "heading_hierarchy": [
    "Lost in the Middle: How Language Models Use Long Contexts",
    "5 Is More Context Is Always Better? A Case Study With Open-Domain QA"
  ],
  "page_numbers": [
    9,
    10,
    11,
    12
  ],
  "continuation_flag": "False",
  "source_batch": 2,
  "metadata": {
    "position_in_batch": 8,
    "raw_length": 874,
    "heading_count": 2
  }
}